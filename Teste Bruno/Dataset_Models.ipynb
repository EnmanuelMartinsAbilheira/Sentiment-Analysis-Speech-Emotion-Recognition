{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "csv_path = \"E:\\Github\\Sentiment-Analysis-Speech-Emotion-Recognition\\Dataset\\dataset.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "print(\"Dataset Overview:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "plt.figure(figsize=(6, 2))\n",
    "sns.countplot(data=df, x=\"Type\")\n",
    "plt.title(\"Distribution of Type Labels\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 2))\n",
    "sns.countplot(data=df, x=\"Factuality\")\n",
    "plt.title(\"Distribution of Factuality Labels\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 2))\n",
    "sns.countplot(data=df, x=\"Sentiment\")\n",
    "plt.title(\"Distribution of Sentiment Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embeddings = embedding_model.encode(df[\"Sentence\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Step 1: Fit on a large corpus\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "corpus = df[\"Sentence\"].tolist()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Step 2: Fix the vocabulary\n",
    "fixed_vocab = vectorizer.get_feature_names_out()\n",
    "vectorizer_fixed = TfidfVectorizer(vocabulary=fixed_vocab)\n",
    "\n",
    "# Fit and transform the corpus using the fixed vocabulary\n",
    "vectorizer_fixed.fit(corpus)  # This ensures the vectorizer is \"fitted\"\n",
    "X_tfidf = vectorizer_fixed.transform(corpus)\n",
    "\n",
    "print(X_tfidf.shape)  # (num_samples, 5000)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer_fixed, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "type_mapping = {\"Affirmation\": 0, \"Negation\": 1}\n",
    "fact_subj_mapping = {\"Factual\": 0, \"Subjective\": 1}\n",
    "sentiment_mapping = {\"Sadness\": 0, \"Anger\": 1, \"Neutral\": 2, \"Happiness\": 3, \"Euphoria\": 4}\n",
    "\n",
    "df[\"Type\"] = df[\"Type\"].map(type_mapping).fillna(-1).astype(int)\n",
    "df[\"Factuality\"] = df[\"Factuality\"].map(fact_subj_mapping).fillna(-1).astype(int)\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].map(sentiment_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "type_labels = df[\"Type\"].values\n",
    "fact_subj_labels = df[\"Factuality\"].values\n",
    "sentiment_labels = df[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "def split_data(X):\n",
    "    return train_test_split(X, type_labels, fact_subj_labels, sentiment_labels, test_size=0.2, random_state=42, stratify=sentiment_labels)\n",
    "\n",
    "X_train_emb, X_test_emb, y_type_train, y_type_test, y_fact_train, y_fact_test, y_sent_train, y_sent_test = split_data(embeddings)\n",
    "X_train_tfidf, X_test_tfidf, _, _, _, _, _, _ = split_data(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grids\n",
    "logistic_param_grid = {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [200, 500, 1000, 1500], \n",
    "    \"max_depth\": [3, 5, 7, 9, 12],  \n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.3],  \n",
    "    \"min_child_weight\": [1, 3, 5, 7],  \n",
    "    \"subsample\": [0.6, 0.8, 1.0],  \n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0]  \n",
    "}\n",
    "\n",
    "# xgb_param_grid = {\"n_estimators\": [100], \"max_depth\": [3], \"learning_rate\": [0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Functions\n",
    "# def train_model(model, param_grid, X_train, y_train, X_test, y_test, name):\n",
    "#     grid_search = GridSearchCV(model, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     print(f\"Best Model for {name}: {grid_search.best_params_}\")\n",
    "#     print(f\"Accuracy for {name}: {acc:.4f}\\n\")\n",
    "#     return best_model, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions\n",
    "def train_model(model, param_distributions, X_train, y_train, X_test, y_test, name, n_iter=10):\n",
    "    random_search = RandomizedSearchCV(model, param_distributions, n_iter=n_iter, cv=3, scoring=\"accuracy\", n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Best Model for {name}: {random_search.best_params_}\")\n",
    "    print(f\"Accuracy for {name}: {acc:.4f}\\n\")\n",
    "    return best_model, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Store the best models\n",
    "best_models = {}\n",
    "\n",
    "# Run all combinations and store results\n",
    "for feature_name, (X_train, X_test) in zip([\"Embedding\", \"TF-IDF\"], [(X_train_emb, X_test_emb), (X_train_tfidf, X_test_tfidf)]):\n",
    "    for model_name, (model, param_grid) in zip([\"XGBoost\", \"Logistic Regression\"], [(XGBClassifier(eval_metric=\"mlogloss\"), xgb_param_grid), (LogisticRegression(max_iter=1000), logistic_param_grid)]):\n",
    "        for task_name, (y_train, y_test) in zip([\"Type\", \"Factuality\", \"Sentiment\"], [(y_type_train, y_type_test), (y_fact_train, y_fact_test), (y_sent_train, y_sent_test)]):\n",
    "            # Train the model\n",
    "            trained_model, acc = train_model(model, param_grid, X_train, y_train, X_test, y_test, f\"{task_name} ({feature_name} + {model_name})\")\n",
    "            \n",
    "            # Predict with the trained model\n",
    "            y_pred = trained_model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "            recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            # Store result in the results list\n",
    "            results.append((feature_name, model_name, task_name, acc, precision, recall, f1, conf_matrix))\n",
    "            \n",
    "            # Check if this model is the best for the current task based on accuracy\n",
    "            if task_name not in best_models or acc > best_models[task_name][\"accuracy\"]:\n",
    "                best_models[task_name] = {\n",
    "                    \"feature_name\": feature_name,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1\": f1,\n",
    "                    \"conf_matrix\": conf_matrix,\n",
    "                    \"trained_model\": trained_model\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame with 8 columns (including confusion matrix)\n",
    "results_df = pd.DataFrame(results, columns=[\"Feature\", \"Model\", \"Task\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ConfMatrix\"])\n",
    "\n",
    "tasks = [\"Type\", \"Factuality\", \"Sentiment\"]\n",
    "colors = [\"Set1\", \"Set2\", \"Set3\"]\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, len(tasks), figsize=(15, 5))\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    task_df = results_df[results_df[\"Task\"] == task]\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Create the barplot\n",
    "    barplot = sns.barplot(\n",
    "        data=task_df, \n",
    "        x=\"Model\", \n",
    "        y=\"Accuracy\", \n",
    "        hue=\"Feature\", \n",
    "        dodge=True, \n",
    "        palette=colors[i], \n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for p in barplot.patches:\n",
    "        ax.annotate(f\"{p.get_height():.3f}\", \n",
    "                    (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                    ha='center', va='bottom', fontsize=10, color='black')\n",
    "    \n",
    "    # Set title and axis labels\n",
    "    ax.set_title(f\" {task}\", fontsize=14)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(\"Model\", fontsize=12)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    \n",
    "    # Add gridlines\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Customize legend\n",
    "    ax.legend(title=\"Feature Extraction\", title_fontsize=12, fontsize=10, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the summary of the best models and display confusion matrix heatmap\n",
    "for task_name, model_info in best_models.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{'Best model for ' + task_name:^38}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    # Prepare the table for a clean display\n",
    "    table = [\n",
    "        [\"Feature\", model_info['feature_name']],\n",
    "        [\"Model\", model_info['model_name']],\n",
    "        [\"Accuracy\", f\"{model_info['accuracy']:.4f}\"],\n",
    "        [\"Precision\", f\"{model_info['precision']:.4f}\"],\n",
    "        [\"Recall\", f\"{model_info['recall']:.4f}\"],\n",
    "        [\"F1 Score\", f\"{model_info['f1']:.4f}\"],\n",
    "    ]\n",
    "    \n",
    "    # Display the table neatly\n",
    "    print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))\n",
    "    \n",
    "    # Plot the confusion matrix using a heatmap\n",
    "    conf_matrix = model_info['conf_matrix']\n",
    "    \n",
    "    # Determine the labels based on the task_name\n",
    "    if task_name == \"Type\":\n",
    "        labels = list(type_mapping.keys())\n",
    "    elif task_name == \"Factuality\":\n",
    "        labels = list(fact_subj_mapping.keys())\n",
    "    elif task_name == \"Sentiment\":\n",
    "        labels = list(sentiment_mapping.keys())\n",
    "    else:\n",
    "        # Default to numeric labels if no mapping is found\n",
    "        labels = list(range(conf_matrix.shape[0]))  # Use the number of classes in the confusion matrix\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, square=True,\n",
    "                xticklabels=labels, yticklabels=labels, linewidths=0.5)\n",
    "    \n",
    "    plt.title(f\"Confusion Matrix for {task_name}\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*40}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the best models\n",
    "for task_name, model_info in best_models.items():\n",
    "    model_filename = f\"best_model_{task_name}.pkl\"\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model_info[\"trained_model\"], f)\n",
    "    print(f\"Best model for {task_name} saved as {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
