{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, RNN, LSTMCell, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Loads precomputed mel-spectrogram images and extracts labels from folder names.\n",
    "    Returns both raw data and data split into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(dataset_path))  # Get emotion categories\n",
    "    \n",
    "    for label in class_names:\n",
    "        class_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        files = glob.glob(os.path.join(class_path, '*.png'))\n",
    "        print(f\"Found {len(files)} images for class '{label}'.\")\n",
    "        \n",
    "        for file in files:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "            img = cv2.resize(img, img_size)  # Resize to standard size\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "    y = np.array(y)\n",
    "    return X, y, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, augmentation_factor=0.3):\n",
    "    \"\"\"Apply simple data augmentation to improve model generalization\"\"\"\n",
    "    aug_X = []\n",
    "    aug_y = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        # Original sample\n",
    "        aug_X.append(X[i])\n",
    "        aug_y.append(y[i])\n",
    "        \n",
    "        # Apply time stretching (horizontal scaling)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            stretched = cv2.resize(X[i], (int(X[i].shape[1] * 1.1), X[i].shape[0]))\n",
    "            stretched = cv2.resize(stretched, (X[i].shape[1], X[i].shape[0]))\n",
    "            aug_X.append(stretched)\n",
    "            aug_y.append(y[i])\n",
    "        \n",
    "        # Apply frequency masking (random horizontal lines masked)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            masked = X[i].copy()\n",
    "            num_masks = np.random.randint(1, 4)\n",
    "            for _ in range(num_masks):\n",
    "                freq_width = np.random.randint(5, 15)\n",
    "                freq_start = np.random.randint(0, X[i].shape[0] - freq_width)\n",
    "                masked[freq_start:freq_start+freq_width, :] = 0\n",
    "            aug_X.append(masked)\n",
    "            aug_y.append(y[i])\n",
    "    \n",
    "    return np.array(aug_X), np.array(aug_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(y):\n",
    "    \"\"\"Encodes string labels into numerical one-hot vectors.\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "    return y_onehot, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a hybrid CNN-LSTM model that leverages both spatial and temporal features.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First, add CNN layers to extract spatial features\n",
    "    model.add(Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Reshape for LSTM - treat rows as time steps\n",
    "    model.add(Reshape((32, 32*64)))\n",
    "    \n",
    "    # LSTM layers for temporal features\n",
    "    model.add(RNN(LSTMCell(128), return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(RNN(LSTMCell(64)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Classification layers\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration parameters\n",
    "    dataset_path = 'RAVDESS_mel_spectrograms'  # Update with your dataset path\n",
    "    img_size = (128, 128)\n",
    "    test_size = 0.15\n",
    "    val_size = 0.15\n",
    "    batch_size = 16\n",
    "    max_epochs = 500\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y, class_names = load_data(dataset_path, img_size)\n",
    "    print(f\"Original dataset: {X.shape[0]} samples\")\n",
    "    \n",
    "    # Apply data augmentation\n",
    "    X_aug, y_aug = augment_data(X, y)\n",
    "    print(f\"Augmented dataset: {X_aug.shape[0]} samples\")\n",
    "    \n",
    "    # Encode labels\n",
    "    y_onehot, le = preprocess_labels(y_aug)\n",
    "    \n",
    "    # Create train, validation, and test sets\n",
    "    # First split off the test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_aug, y_onehot, test_size=test_size, random_state=42, \n",
    "        stratify=np.argmax(y_onehot, axis=1)\n",
    "    )\n",
    "    \n",
    "    # Then split the remaining data into train and validation\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, random_state=42,\n",
    "        stratify=np.argmax(y_temp, axis=1)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]}, Validation: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Build and compile the model\n",
    "    input_shape = X_train.shape[1:]  # (128, 128)\n",
    "    num_classes = y_onehot.shape[1]\n",
    "    model = build_hybrid_model(input_shape, num_classes)\n",
    "    \n",
    "    # Use a learning rate schedule with warmup\n",
    "    initial_lr = 0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks for training\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_emotion_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stop, checkpoint, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    # Generate predictions and evaluate in detail\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true_classes, \n",
    "        y_pred_classes, \n",
    "        target_names=[le.inverse_transform([i])[0] for i in range(num_classes)]\n",
    "    ))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
