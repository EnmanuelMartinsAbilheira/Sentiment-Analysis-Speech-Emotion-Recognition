{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Resizing, Lambda, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path, target_size=(224, 224)):\n",
    "    X, y = [], []\n",
    "    class_labels = sorted(os.listdir(dataset_path))  # Extract labels from folder names\n",
    "    print(f\"Detected classes: {class_labels}\")\n",
    "    \n",
    "    for label in class_labels:\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        \n",
    "        for file in os.listdir(label_path):\n",
    "            file_path = os.path.join(label_path, file)\n",
    "            try:\n",
    "                img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "                img = cv2.resize(img, target_size)  # Resize to match model input\n",
    "                img = np.stack([img] * 3, axis=-1)  # Convert to 3 channels\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    X = np.array(X) / 255.0  # Normalize pixel values\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "    return y_onehot, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 100 == 0:\n",
    "        return lr * 0.5\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    # Build a transfer learning model using VGG16 pre-trained on ImageNet.\n",
    "    # VGG16 expects images of shape 224x224x3.\n",
    "    model = Sequential()\n",
    "    # Resize input spectrograms to 224x224.\n",
    "    model.add(Resizing(224, 224, input_shape=input_shape))\n",
    "    # Apply VGG16 preprocessing.\n",
    "    model.add(Lambda(tf.keras.applications.vgg16.preprocess_input))\n",
    "    \n",
    "    # Load the VGG16 base model with pre-trained weights.\n",
    "    base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    # Freeze the base model layers.\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.add(base_model)\n",
    "    \n",
    "    # Replace Flatten with Global Average Pooling for better generalization.\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_path = 'RAVDESS_mel_spectrograms'  # Update with your dataset directory\n",
    "    X, y = load_data(dataset_path)\n",
    "    \n",
    "    y_onehot, le = preprocess_labels(y)\n",
    "    print(\"Detected labels:\", le.classes_)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_onehot, test_size=0.2, random_state=42, stratify=np.argmax(y_onehot, axis=1)\n",
    "    )\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "    num_classes = y_onehot.shape[1]\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True)\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, min_lr=1e-6)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train, epochs=500, batch_size=32, validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stop, lr_scheduler, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    loss, acc = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Loss: {loss:.4f}, Validation Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # model.save('sentiment_inception_model.h5')\n",
    "    # print(\"Model saved as 'sentiment_inception_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
