{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Dropout, Dense, RNN, LSTMCell\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Loads precomputed mel-spectrogram images and extracts labels from folder names.\n",
    "    Each image is resized to (128, 128) and normalized.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(dataset_path))  # Get emotion categories\n",
    "    \n",
    "    for label in class_names:\n",
    "        class_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        files = glob.glob(os.path.join(class_path, '*.png'))\n",
    "        print(f\"Found {len(files)} images for class '{label}'.\")\n",
    "        \n",
    "        for file in files:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    y = np.array(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(y):\n",
    "    \"\"\"\n",
    "    Encodes string labels into numerical one-hot vectors.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "    return y_onehot, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds and returns a model with three Conv1D layers followed by three LSTM layers \n",
    "    and several Dense layers for classification.\n",
    "    \n",
    "    The input_shape is expected to be (timesteps, features), for example (128, 128).\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv1D block\n",
    "    model.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Second Conv1D block\n",
    "    model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Third Conv1D block\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # LSTM layers using LSTMCell wrapped in RNN to avoid cuDNN issues on AMD GPUs with DirectML\n",
    "    model.add(RNN(LSTMCell(128), return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(RNN(LSTMCell(128), return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(RNN(LSTMCell(128)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Fully connected Dense layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    \"\"\"Simple learning rate scheduler that halves the lr every 100 epochs.\"\"\"\n",
    "    return lr * 0.5 if epoch > 0 and epoch % 100 == 0 else lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_path = 'RAVDESS_mel_spectrograms'  # Update with your dataset path\n",
    "    X, y = load_data(dataset_path)\n",
    "    # X has shape (num_samples, 128, 128). Each sample is treated as a sequence of 128 time steps,\n",
    "    # each with 128 features.\n",
    "    \n",
    "    y_onehot, le = preprocess_labels(y)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_onehot, test_size=0.2, random_state=42, stratify=np.argmax(y_onehot, axis=1)\n",
    "    )\n",
    "    \n",
    "    input_shape = X_train.shape[1:]  # Expected to be (128, 128)\n",
    "    num_classes = y_onehot.shape[1]\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True)\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=100, min_lr=1e-6)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=2000,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stop, lr_scheduler, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    loss, acc = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Loss: {loss:.4f}, Validation Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192 images for class 'angry'.\n",
      "Found 192 images for class 'calm'.\n",
      "Found 192 images for class 'disgust'.\n",
      "Found 192 images for class 'fearful'.\n",
      "Found 192 images for class 'happy'.\n",
      "Found 96 images for class 'neutral'.\n",
      "Found 192 images for class 'sad'.\n",
      "Found 192 images for class 'surprised'.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 128, 1024)         656384    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 64, 1024)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 1024)         4096      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 1024)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 64, 512)           2621952   \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 32, 512)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32, 512)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 32, 256)           655616    \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 16, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 256)           0         \n",
      "                                                                 \n",
      " rnn (RNN)                   (None, 16, 128)           197120    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 128)           0         \n",
      "                                                                 \n",
      " rnn_1 (RNN)                 (None, 16, 128)           131584    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16, 128)           0         \n",
      "                                                                 \n",
      " rnn_2 (RNN)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,428,520\n",
      "Trainable params: 4,424,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "36/36 [==============================] - 4s 61ms/step - loss: 1.9737 - accuracy: 0.2135 - val_loss: 2.0003 - val_accuracy: 0.2257 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 1.8414 - accuracy: 0.2925 - val_loss: 1.9930 - val_accuracy: 0.2153 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 1.7517 - accuracy: 0.3351 - val_loss: 2.1282 - val_accuracy: 0.1840 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 1.7035 - accuracy: 0.3438 - val_loss: 2.3782 - val_accuracy: 0.1701 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 1.5669 - accuracy: 0.4288 - val_loss: 1.7995 - val_accuracy: 0.3264 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 1.5231 - accuracy: 0.4410 - val_loss: 1.8848 - val_accuracy: 0.2882 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 1.4555 - accuracy: 0.4731 - val_loss: 1.6657 - val_accuracy: 0.3681 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 1.4155 - accuracy: 0.4766 - val_loss: 1.9012 - val_accuracy: 0.2604 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 1.3624 - accuracy: 0.4983 - val_loss: 1.6392 - val_accuracy: 0.3993 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 1.3776 - accuracy: 0.5052 - val_loss: 2.0846 - val_accuracy: 0.3299 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 1.2934 - accuracy: 0.5339 - val_loss: 1.7120 - val_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 1.3065 - accuracy: 0.5339 - val_loss: 1.7489 - val_accuracy: 0.3715 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 1.2332 - accuracy: 0.5582 - val_loss: 1.7002 - val_accuracy: 0.3681 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 1.2753 - accuracy: 0.5304 - val_loss: 1.9995 - val_accuracy: 0.3264 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 1.1493 - accuracy: 0.5747 - val_loss: 1.8347 - val_accuracy: 0.4236 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 1.0917 - accuracy: 0.6033 - val_loss: 1.5998 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 1.1221 - accuracy: 0.5938 - val_loss: 1.9777 - val_accuracy: 0.3646 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 1.0364 - accuracy: 0.6172 - val_loss: 1.6587 - val_accuracy: 0.3889 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 1.0399 - accuracy: 0.6189 - val_loss: 1.9635 - val_accuracy: 0.3681 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.9935 - accuracy: 0.6571 - val_loss: 1.8360 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.9354 - accuracy: 0.6684 - val_loss: 1.8137 - val_accuracy: 0.4097 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.8754 - accuracy: 0.6962 - val_loss: 2.0612 - val_accuracy: 0.4375 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.9212 - accuracy: 0.6710 - val_loss: 1.6596 - val_accuracy: 0.4861 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.9098 - accuracy: 0.6753 - val_loss: 1.8630 - val_accuracy: 0.4340 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.8335 - accuracy: 0.7127 - val_loss: 1.6252 - val_accuracy: 0.4896 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "36/36 [==============================] - 2s 61ms/step - loss: 0.8163 - accuracy: 0.7066 - val_loss: 1.7705 - val_accuracy: 0.4583 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.7454 - accuracy: 0.7378 - val_loss: 1.9032 - val_accuracy: 0.4028 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.7369 - accuracy: 0.7439 - val_loss: 2.0098 - val_accuracy: 0.4479 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.7082 - accuracy: 0.7352 - val_loss: 2.0704 - val_accuracy: 0.4167 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.6917 - accuracy: 0.7613 - val_loss: 2.2160 - val_accuracy: 0.3681 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.7113 - accuracy: 0.7500 - val_loss: 1.6543 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.6348 - accuracy: 0.7786 - val_loss: 1.9050 - val_accuracy: 0.4861 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.6347 - accuracy: 0.7708 - val_loss: 1.8545 - val_accuracy: 0.4340 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.6197 - accuracy: 0.7873 - val_loss: 1.7693 - val_accuracy: 0.5104 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.5066 - accuracy: 0.8281 - val_loss: 2.2351 - val_accuracy: 0.4931 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.5645 - accuracy: 0.8212 - val_loss: 2.0792 - val_accuracy: 0.4549 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.5164 - accuracy: 0.8160 - val_loss: 1.6652 - val_accuracy: 0.5104 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.4494 - accuracy: 0.8438 - val_loss: 1.7570 - val_accuracy: 0.5278 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.4388 - accuracy: 0.8464 - val_loss: 2.1124 - val_accuracy: 0.4896 - lr: 0.0010\n",
      "Epoch 40/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.4846 - accuracy: 0.8247 - val_loss: 1.7539 - val_accuracy: 0.5208 - lr: 0.0010\n",
      "Epoch 41/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.4978 - accuracy: 0.8281 - val_loss: 1.5606 - val_accuracy: 0.5139 - lr: 0.0010\n",
      "Epoch 42/2000\n",
      "36/36 [==============================] - 2s 59ms/step - loss: 0.3811 - accuracy: 0.8776 - val_loss: 1.6600 - val_accuracy: 0.5243 - lr: 0.0010\n",
      "Epoch 43/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.3495 - accuracy: 0.8819 - val_loss: 1.8141 - val_accuracy: 0.5382 - lr: 0.0010\n",
      "Epoch 44/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.3903 - accuracy: 0.8602 - val_loss: 2.1083 - val_accuracy: 0.4722 - lr: 0.0010\n",
      "Epoch 45/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.4780 - accuracy: 0.8411 - val_loss: 1.6413 - val_accuracy: 0.5035 - lr: 0.0010\n",
      "Epoch 46/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.4025 - accuracy: 0.8611 - val_loss: 1.7948 - val_accuracy: 0.5417 - lr: 0.0010\n",
      "Epoch 47/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.4027 - accuracy: 0.8724 - val_loss: 2.0644 - val_accuracy: 0.4514 - lr: 0.0010\n",
      "Epoch 48/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.3431 - accuracy: 0.8854 - val_loss: 2.2124 - val_accuracy: 0.5035 - lr: 0.0010\n",
      "Epoch 49/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.3093 - accuracy: 0.9054 - val_loss: 1.8593 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 50/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.3325 - accuracy: 0.8976 - val_loss: 1.8732 - val_accuracy: 0.5382 - lr: 0.0010\n",
      "Epoch 51/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.3244 - accuracy: 0.8967 - val_loss: 2.0444 - val_accuracy: 0.5208 - lr: 0.0010\n",
      "Epoch 52/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.3467 - accuracy: 0.8837 - val_loss: 1.8534 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 53/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.3123 - accuracy: 0.8967 - val_loss: 1.9085 - val_accuracy: 0.5243 - lr: 0.0010\n",
      "Epoch 54/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.2582 - accuracy: 0.9115 - val_loss: 1.9740 - val_accuracy: 0.5521 - lr: 0.0010\n",
      "Epoch 55/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.2421 - accuracy: 0.9175 - val_loss: 2.5374 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 56/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.2598 - accuracy: 0.9123 - val_loss: 2.7243 - val_accuracy: 0.4757 - lr: 0.0010\n",
      "Epoch 57/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.3160 - accuracy: 0.8872 - val_loss: 2.2499 - val_accuracy: 0.5243 - lr: 0.0010\n",
      "Epoch 58/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.3130 - accuracy: 0.9045 - val_loss: 1.9546 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 59/2000\n",
      "36/36 [==============================] - 2s 64ms/step - loss: 0.2261 - accuracy: 0.9297 - val_loss: 1.9920 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 60/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.2377 - accuracy: 0.9227 - val_loss: 1.7120 - val_accuracy: 0.5729 - lr: 0.0010\n",
      "Epoch 61/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.1992 - accuracy: 0.9375 - val_loss: 2.0921 - val_accuracy: 0.5486 - lr: 0.0010\n",
      "Epoch 62/2000\n",
      "36/36 [==============================] - 3s 91ms/step - loss: 0.2374 - accuracy: 0.9245 - val_loss: 1.7944 - val_accuracy: 0.5729 - lr: 0.0010\n",
      "Epoch 63/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.2528 - accuracy: 0.9123 - val_loss: 2.1878 - val_accuracy: 0.4965 - lr: 0.0010\n",
      "Epoch 64/2000\n",
      "36/36 [==============================] - 3s 92ms/step - loss: 0.2107 - accuracy: 0.9332 - val_loss: 2.2167 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 65/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.2073 - accuracy: 0.9306 - val_loss: 2.0976 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 66/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.2522 - accuracy: 0.9210 - val_loss: 2.1633 - val_accuracy: 0.5069 - lr: 0.0010\n",
      "Epoch 67/2000\n",
      "36/36 [==============================] - 3s 91ms/step - loss: 0.2114 - accuracy: 0.9297 - val_loss: 1.8657 - val_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 68/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.1903 - accuracy: 0.9401 - val_loss: 2.2637 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 69/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.1359 - accuracy: 0.9566 - val_loss: 1.8614 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 70/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.1496 - accuracy: 0.9488 - val_loss: 2.3486 - val_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 71/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.1885 - accuracy: 0.9401 - val_loss: 2.2156 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 72/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.2326 - accuracy: 0.9149 - val_loss: 2.4607 - val_accuracy: 0.5104 - lr: 0.0010\n",
      "Epoch 73/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.1506 - accuracy: 0.9523 - val_loss: 1.9271 - val_accuracy: 0.6042 - lr: 0.0010\n",
      "Epoch 74/2000\n",
      "36/36 [==============================] - 3s 92ms/step - loss: 0.1211 - accuracy: 0.9618 - val_loss: 2.0326 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 75/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.1204 - accuracy: 0.9644 - val_loss: 2.4360 - val_accuracy: 0.5799 - lr: 0.0010\n",
      "Epoch 76/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.1190 - accuracy: 0.9566 - val_loss: 2.5307 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 77/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.1777 - accuracy: 0.9462 - val_loss: 2.4568 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 78/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.1848 - accuracy: 0.9366 - val_loss: 2.1589 - val_accuracy: 0.5590 - lr: 0.0010\n",
      "Epoch 79/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.1661 - accuracy: 0.9497 - val_loss: 2.2747 - val_accuracy: 0.5382 - lr: 0.0010\n",
      "Epoch 80/2000\n",
      "36/36 [==============================] - 3s 94ms/step - loss: 0.1732 - accuracy: 0.9497 - val_loss: 1.9295 - val_accuracy: 0.5590 - lr: 0.0010\n",
      "Epoch 81/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.1525 - accuracy: 0.9540 - val_loss: 1.7653 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 82/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.1062 - accuracy: 0.9618 - val_loss: 1.9548 - val_accuracy: 0.6042 - lr: 0.0010\n",
      "Epoch 83/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.1296 - accuracy: 0.9583 - val_loss: 2.1856 - val_accuracy: 0.5799 - lr: 0.0010\n",
      "Epoch 84/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.2664 - accuracy: 0.9253 - val_loss: 1.8039 - val_accuracy: 0.5868 - lr: 0.0010\n",
      "Epoch 85/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.1823 - accuracy: 0.9401 - val_loss: 1.7614 - val_accuracy: 0.5903 - lr: 0.0010\n",
      "Epoch 86/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.1450 - accuracy: 0.9557 - val_loss: 2.5911 - val_accuracy: 0.5208 - lr: 0.0010\n",
      "Epoch 87/2000\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 0.1293 - accuracy: 0.9583 - val_loss: 2.2457 - val_accuracy: 0.5243 - lr: 0.0010\n",
      "Epoch 88/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.1278 - accuracy: 0.9549 - val_loss: 2.0841 - val_accuracy: 0.5417 - lr: 0.0010\n",
      "Epoch 89/2000\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 0.1707 - accuracy: 0.9392 - val_loss: 1.8585 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 90/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.1365 - accuracy: 0.9583 - val_loss: 2.1282 - val_accuracy: 0.5590 - lr: 0.0010\n",
      "Epoch 91/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.0960 - accuracy: 0.9661 - val_loss: 2.6338 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 92/2000\n",
      "36/36 [==============================] - 3s 93ms/step - loss: 0.1392 - accuracy: 0.9505 - val_loss: 2.4611 - val_accuracy: 0.5417 - lr: 0.0010\n",
      "Epoch 93/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.1224 - accuracy: 0.9583 - val_loss: 2.2717 - val_accuracy: 0.5729 - lr: 0.0010\n",
      "Epoch 94/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.1458 - accuracy: 0.9601 - val_loss: 2.6346 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 95/2000\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 0.2123 - accuracy: 0.9410 - val_loss: 2.2196 - val_accuracy: 0.4861 - lr: 0.0010\n",
      "Epoch 96/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.1188 - accuracy: 0.9627 - val_loss: 2.2169 - val_accuracy: 0.5590 - lr: 0.0010\n",
      "Epoch 97/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.1133 - accuracy: 0.9653 - val_loss: 2.1523 - val_accuracy: 0.5764 - lr: 0.0010\n",
      "Epoch 98/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.1009 - accuracy: 0.9722 - val_loss: 2.1706 - val_accuracy: 0.5764 - lr: 0.0010\n",
      "Epoch 99/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0841 - accuracy: 0.9714 - val_loss: 1.9996 - val_accuracy: 0.5799 - lr: 0.0010\n",
      "Epoch 100/2000\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.1274 - accuracy: 0.9661 - val_loss: 2.1718 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 101/2000\n",
      "36/36 [==============================] - 3s 87ms/step - loss: 0.0914 - accuracy: 0.9722 - val_loss: 2.1132 - val_accuracy: 0.5938 - lr: 5.0000e-04\n",
      "Epoch 102/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0665 - accuracy: 0.9852 - val_loss: 1.9723 - val_accuracy: 0.6111 - lr: 5.0000e-04\n",
      "Epoch 103/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0388 - accuracy: 0.9835 - val_loss: 2.0748 - val_accuracy: 0.6285 - lr: 5.0000e-04\n",
      "Epoch 104/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0392 - accuracy: 0.9870 - val_loss: 2.0987 - val_accuracy: 0.6146 - lr: 5.0000e-04\n",
      "Epoch 105/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0379 - accuracy: 0.9905 - val_loss: 2.0614 - val_accuracy: 0.6285 - lr: 5.0000e-04\n",
      "Epoch 106/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0269 - accuracy: 0.9905 - val_loss: 2.1003 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 107/2000\n",
      "36/36 [==============================] - 2s 53ms/step - loss: 0.0297 - accuracy: 0.9905 - val_loss: 2.2006 - val_accuracy: 0.6007 - lr: 5.0000e-04\n",
      "Epoch 108/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 0.0390 - accuracy: 0.9887 - val_loss: 2.1090 - val_accuracy: 0.6424 - lr: 5.0000e-04\n",
      "Epoch 109/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0233 - accuracy: 0.9948 - val_loss: 2.2012 - val_accuracy: 0.6493 - lr: 5.0000e-04\n",
      "Epoch 110/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0449 - accuracy: 0.9870 - val_loss: 2.4479 - val_accuracy: 0.6076 - lr: 5.0000e-04\n",
      "Epoch 111/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0265 - accuracy: 0.9948 - val_loss: 2.2578 - val_accuracy: 0.6076 - lr: 5.0000e-04\n",
      "Epoch 112/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0484 - accuracy: 0.9852 - val_loss: 2.2724 - val_accuracy: 0.5972 - lr: 5.0000e-04\n",
      "Epoch 113/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0196 - accuracy: 0.9948 - val_loss: 2.1835 - val_accuracy: 0.6111 - lr: 5.0000e-04\n",
      "Epoch 114/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0185 - accuracy: 0.9948 - val_loss: 2.1898 - val_accuracy: 0.6215 - lr: 5.0000e-04\n",
      "Epoch 115/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0361 - accuracy: 0.9887 - val_loss: 2.4600 - val_accuracy: 0.5903 - lr: 5.0000e-04\n",
      "Epoch 116/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0230 - accuracy: 0.9939 - val_loss: 2.3252 - val_accuracy: 0.5972 - lr: 5.0000e-04\n",
      "Epoch 117/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0209 - accuracy: 0.9957 - val_loss: 2.2341 - val_accuracy: 0.6250 - lr: 5.0000e-04\n",
      "Epoch 118/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0308 - accuracy: 0.9905 - val_loss: 2.3663 - val_accuracy: 0.6111 - lr: 5.0000e-04\n",
      "Epoch 119/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0283 - accuracy: 0.9913 - val_loss: 2.3796 - val_accuracy: 0.6111 - lr: 5.0000e-04\n",
      "Epoch 120/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0394 - accuracy: 0.9887 - val_loss: 2.3444 - val_accuracy: 0.5972 - lr: 5.0000e-04\n",
      "Epoch 121/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0268 - accuracy: 0.9896 - val_loss: 2.2965 - val_accuracy: 0.6146 - lr: 5.0000e-04\n",
      "Epoch 122/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 0.0172 - accuracy: 0.9957 - val_loss: 2.2942 - val_accuracy: 0.6493 - lr: 5.0000e-04\n",
      "Epoch 123/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0174 - accuracy: 0.9957 - val_loss: 2.2273 - val_accuracy: 0.6389 - lr: 5.0000e-04\n",
      "Epoch 124/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0078 - accuracy: 0.9983 - val_loss: 2.3190 - val_accuracy: 0.6354 - lr: 5.0000e-04\n",
      "Epoch 125/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0298 - accuracy: 0.9913 - val_loss: 2.4391 - val_accuracy: 0.6493 - lr: 5.0000e-04\n",
      "Epoch 126/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0424 - accuracy: 0.9870 - val_loss: 2.5115 - val_accuracy: 0.5903 - lr: 5.0000e-04\n",
      "Epoch 127/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0470 - accuracy: 0.9887 - val_loss: 2.6228 - val_accuracy: 0.5729 - lr: 5.0000e-04\n",
      "Epoch 128/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0265 - accuracy: 0.9913 - val_loss: 2.2416 - val_accuracy: 0.6493 - lr: 5.0000e-04\n",
      "Epoch 129/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 0.0217 - accuracy: 0.9939 - val_loss: 2.5266 - val_accuracy: 0.6076 - lr: 5.0000e-04\n",
      "Epoch 130/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0336 - accuracy: 0.9887 - val_loss: 2.5051 - val_accuracy: 0.6007 - lr: 5.0000e-04\n",
      "Epoch 131/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 2.7681 - val_accuracy: 0.5938 - lr: 5.0000e-04\n",
      "Epoch 132/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 2.6575 - val_accuracy: 0.6111 - lr: 5.0000e-04\n",
      "Epoch 133/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0273 - accuracy: 0.9878 - val_loss: 2.6660 - val_accuracy: 0.6042 - lr: 5.0000e-04\n",
      "Epoch 134/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0345 - accuracy: 0.9913 - val_loss: 2.3977 - val_accuracy: 0.6458 - lr: 5.0000e-04\n",
      "Epoch 135/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0494 - accuracy: 0.9852 - val_loss: 2.2660 - val_accuracy: 0.6146 - lr: 5.0000e-04\n",
      "Epoch 136/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0516 - accuracy: 0.9887 - val_loss: 2.6071 - val_accuracy: 0.5799 - lr: 5.0000e-04\n",
      "Epoch 137/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0383 - accuracy: 0.9844 - val_loss: 2.4200 - val_accuracy: 0.6111 - lr: 5.0000e-04\n",
      "Epoch 138/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0458 - accuracy: 0.9844 - val_loss: 2.3389 - val_accuracy: 0.6319 - lr: 5.0000e-04\n",
      "Epoch 139/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0435 - accuracy: 0.9818 - val_loss: 2.2571 - val_accuracy: 0.6424 - lr: 5.0000e-04\n",
      "Epoch 140/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0398 - accuracy: 0.9870 - val_loss: 2.2957 - val_accuracy: 0.6146 - lr: 5.0000e-04\n",
      "Epoch 141/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0254 - accuracy: 0.9922 - val_loss: 2.2832 - val_accuracy: 0.6424 - lr: 5.0000e-04\n",
      "Epoch 142/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0183 - accuracy: 0.9922 - val_loss: 2.1888 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 143/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0157 - accuracy: 0.9957 - val_loss: 2.2366 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 144/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 2.2369 - val_accuracy: 0.6528 - lr: 2.5000e-04\n",
      "Epoch 145/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0131 - accuracy: 0.9948 - val_loss: 2.2581 - val_accuracy: 0.6424 - lr: 2.5000e-04\n",
      "Epoch 146/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 2.2192 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 147/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0106 - accuracy: 0.9957 - val_loss: 2.2270 - val_accuracy: 0.6354 - lr: 2.5000e-04\n",
      "Epoch 148/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0170 - accuracy: 0.9957 - val_loss: 2.2424 - val_accuracy: 0.6111 - lr: 2.5000e-04\n",
      "Epoch 149/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0166 - accuracy: 0.9957 - val_loss: 2.2790 - val_accuracy: 0.6285 - lr: 2.5000e-04\n",
      "Epoch 150/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0131 - accuracy: 0.9948 - val_loss: 2.3117 - val_accuracy: 0.6493 - lr: 2.5000e-04\n",
      "Epoch 151/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 2.4611 - val_accuracy: 0.6354 - lr: 2.5000e-04\n",
      "Epoch 152/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0395 - accuracy: 0.9922 - val_loss: 2.3265 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 153/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 2.3035 - val_accuracy: 0.6424 - lr: 2.5000e-04\n",
      "Epoch 154/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 2.3295 - val_accuracy: 0.6528 - lr: 2.5000e-04\n",
      "Epoch 155/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0167 - accuracy: 0.9965 - val_loss: 2.3920 - val_accuracy: 0.6354 - lr: 2.5000e-04\n",
      "Epoch 156/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 2.4303 - val_accuracy: 0.6285 - lr: 2.5000e-04\n",
      "Epoch 157/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0042 - accuracy: 0.9983 - val_loss: 2.4280 - val_accuracy: 0.6458 - lr: 2.5000e-04\n",
      "Epoch 158/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 2.5167 - val_accuracy: 0.6458 - lr: 2.5000e-04\n",
      "Epoch 159/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0059 - accuracy: 0.9974 - val_loss: 2.4705 - val_accuracy: 0.6319 - lr: 2.5000e-04\n",
      "Epoch 160/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 2.4403 - val_accuracy: 0.6389 - lr: 2.5000e-04\n",
      "Epoch 161/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0093 - accuracy: 0.9965 - val_loss: 2.5272 - val_accuracy: 0.6319 - lr: 2.5000e-04\n",
      "Epoch 162/2000\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 0.0137 - accuracy: 0.9948 - val_loss: 2.5255 - val_accuracy: 0.6319 - lr: 2.5000e-04\n",
      "Epoch 163/2000\n",
      "36/36 [==============================] - 3s 79ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 2.6402 - val_accuracy: 0.6285 - lr: 2.5000e-04\n",
      "Epoch 164/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.0218 - accuracy: 0.9948 - val_loss: 2.6552 - val_accuracy: 0.6215 - lr: 2.5000e-04\n",
      "Epoch 165/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.0104 - accuracy: 0.9948 - val_loss: 2.6500 - val_accuracy: 0.6285 - lr: 2.5000e-04\n",
      "Epoch 166/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0154 - accuracy: 0.9957 - val_loss: 2.7613 - val_accuracy: 0.6250 - lr: 2.5000e-04\n",
      "Epoch 167/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 2.5541 - val_accuracy: 0.6458 - lr: 2.5000e-04\n",
      "Epoch 168/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 2.5132 - val_accuracy: 0.6389 - lr: 2.5000e-04\n",
      "Epoch 169/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0153 - accuracy: 0.9957 - val_loss: 2.5307 - val_accuracy: 0.6354 - lr: 2.5000e-04\n",
      "Epoch 170/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 2.5625 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 171/2000\n",
      "36/36 [==============================] - 2s 69ms/step - loss: 0.0146 - accuracy: 0.9957 - val_loss: 2.6529 - val_accuracy: 0.6076 - lr: 2.5000e-04\n",
      "Epoch 172/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0090 - accuracy: 0.9965 - val_loss: 2.8263 - val_accuracy: 0.6181 - lr: 2.5000e-04\n",
      "Epoch 173/2000\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 2.7018 - val_accuracy: 0.6389 - lr: 2.5000e-04\n",
      "Epoch 174/2000\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 2.6620 - val_accuracy: 0.6424 - lr: 2.5000e-04\n",
      "Epoch 175/2000\n",
      "36/36 [==============================] - 30s 859ms/step - loss: 0.0244 - accuracy: 0.9931 - val_loss: 2.8050 - val_accuracy: 0.6111 - lr: 2.5000e-04\n",
      "Epoch 176/2000\n",
      "36/36 [==============================] - 53s 1s/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 2.7150 - val_accuracy: 0.6285 - lr: 2.5000e-04\n",
      "Epoch 177/2000\n",
      "36/36 [==============================] - 57s 2s/step - loss: 0.0159 - accuracy: 0.9965 - val_loss: 2.7753 - val_accuracy: 0.6181 - lr: 2.5000e-04\n",
      "Epoch 178/2000\n",
      "36/36 [==============================] - 56s 2s/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 2.6412 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 179/2000\n",
      "36/36 [==============================] - 56s 2s/step - loss: 0.0337 - accuracy: 0.9922 - val_loss: 2.6472 - val_accuracy: 0.6528 - lr: 2.5000e-04\n",
      "Epoch 180/2000\n",
      "36/36 [==============================] - 56s 2s/step - loss: 0.0123 - accuracy: 0.9948 - val_loss: 2.6265 - val_accuracy: 0.6319 - lr: 2.5000e-04\n",
      "Epoch 181/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 2.5329 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 182/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0121 - accuracy: 0.9957 - val_loss: 2.4739 - val_accuracy: 0.6597 - lr: 2.5000e-04\n",
      "Epoch 183/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0100 - accuracy: 0.9957 - val_loss: 2.5862 - val_accuracy: 0.6632 - lr: 2.5000e-04\n",
      "Epoch 184/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 2.5443 - val_accuracy: 0.6701 - lr: 2.5000e-04\n",
      "Epoch 185/2000\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 2.5346 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 186/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 2.5061 - val_accuracy: 0.6667 - lr: 2.5000e-04\n",
      "Epoch 187/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 2.4595 - val_accuracy: 0.6493 - lr: 2.5000e-04\n",
      "Epoch 188/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 2.5314 - val_accuracy: 0.6389 - lr: 2.5000e-04\n",
      "Epoch 189/2000\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 2.5828 - val_accuracy: 0.6424 - lr: 2.5000e-04\n",
      "Epoch 190/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0103 - accuracy: 0.9957 - val_loss: 2.5296 - val_accuracy: 0.6181 - lr: 2.5000e-04\n",
      "Epoch 191/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.6128 - val_accuracy: 0.6215 - lr: 2.5000e-04\n",
      "Epoch 192/2000\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.6317 - val_accuracy: 0.6181 - lr: 2.5000e-04\n",
      "Epoch 193/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 2.6681 - val_accuracy: 0.6250 - lr: 2.5000e-04\n",
      "Epoch 194/2000\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 0.0150 - accuracy: 0.9965 - val_loss: 2.7217 - val_accuracy: 0.6319 - lr: 2.5000e-04\n",
      "Epoch 195/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 2.6395 - val_accuracy: 0.6493 - lr: 2.5000e-04\n",
      "Epoch 196/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 2.5644 - val_accuracy: 0.6562 - lr: 2.5000e-04\n",
      "Epoch 197/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0145 - accuracy: 0.9957 - val_loss: 2.6977 - val_accuracy: 0.6389 - lr: 2.5000e-04\n",
      "Epoch 198/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 2.9055 - val_accuracy: 0.6146 - lr: 2.5000e-04\n",
      "Epoch 199/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 2.7153 - val_accuracy: 0.6389 - lr: 2.5000e-04\n",
      "Epoch 200/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 2.8652 - val_accuracy: 0.6389 - lr: 2.5000e-04\n",
      "Epoch 201/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0172 - accuracy: 0.9957 - val_loss: 2.6000 - val_accuracy: 0.6319 - lr: 1.2500e-04\n",
      "Epoch 202/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0156 - accuracy: 0.9965 - val_loss: 2.6795 - val_accuracy: 0.6458 - lr: 1.2500e-04\n",
      "Epoch 203/2000\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 2.6859 - val_accuracy: 0.6493 - lr: 1.2500e-04\n",
      "Epoch 204/2000\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 2.6519 - val_accuracy: 0.6493 - lr: 1.2500e-04\n",
      "Epoch 205/2000\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 2.5807 - val_accuracy: 0.6562 - lr: 1.2500e-04\n",
      "Epoch 206/2000\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 2.5431 - val_accuracy: 0.6424 - lr: 1.2500e-04\n",
      "Epoch 207/2000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.0041 - accuracy: 0.9983 - val_loss: 2.6182 - val_accuracy: 0.6424 - lr: 1.2500e-04\n",
      "Epoch 208/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 2.6589 - val_accuracy: 0.6285 - lr: 1.2500e-04\n",
      "Epoch 209/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 2.5847 - val_accuracy: 0.6389 - lr: 1.2500e-04\n",
      "Epoch 210/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0138 - accuracy: 0.9965 - val_loss: 2.6996 - val_accuracy: 0.6250 - lr: 1.2500e-04\n",
      "Epoch 211/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.0056 - accuracy: 0.9974 - val_loss: 2.7285 - val_accuracy: 0.6250 - lr: 1.2500e-04\n",
      "Epoch 212/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0053 - accuracy: 0.9974 - val_loss: 2.6506 - val_accuracy: 0.6389 - lr: 1.2500e-04\n",
      "Epoch 213/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0038 - accuracy: 0.9983 - val_loss: 2.6489 - val_accuracy: 0.6458 - lr: 1.2500e-04\n",
      "Epoch 214/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0127 - accuracy: 0.9974 - val_loss: 2.7084 - val_accuracy: 0.6632 - lr: 1.2500e-04\n",
      "Epoch 215/2000\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0108 - accuracy: 0.9965 - val_loss: 2.7536 - val_accuracy: 0.6424 - lr: 1.2500e-04\n",
      "Epoch 216/2000\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0126 - accuracy: 0.9948 - val_loss: 2.7204 - val_accuracy: 0.6354 - lr: 1.2500e-04\n",
      "Epoch 217/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 2.7452 - val_accuracy: 0.6389 - lr: 1.2500e-04\n",
      "Epoch 218/2000\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0114 - accuracy: 0.9957 - val_loss: 2.6289 - val_accuracy: 0.6424 - lr: 1.2500e-04\n",
      "Epoch 219/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.0117 - accuracy: 0.9974 - val_loss: 2.6633 - val_accuracy: 0.6493 - lr: 1.2500e-04\n",
      "Epoch 220/2000\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 2.5848 - val_accuracy: 0.6667 - lr: 1.2500e-04\n",
      "Epoch 221/2000\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0074 - accuracy: 0.9983 - val_loss: 2.5864 - val_accuracy: 0.6632 - lr: 1.2500e-04\n",
      "Epoch 222/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.5962 - val_accuracy: 0.6667 - lr: 1.2500e-04\n",
      "Epoch 223/2000\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 0.0164 - accuracy: 0.9965 - val_loss: 2.6391 - val_accuracy: 0.6528 - lr: 1.2500e-04\n",
      "Epoch 224/2000\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 0.0036 - accuracy: 0.9983 - val_loss: 2.6441 - val_accuracy: 0.6458 - lr: 1.2500e-04\n",
      "Epoch 225/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0050 - accuracy: 0.9974 - val_loss: 2.5518 - val_accuracy: 0.6493 - lr: 1.2500e-04\n",
      "Epoch 226/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 2.5261 - val_accuracy: 0.6493 - lr: 1.2500e-04\n",
      "Epoch 227/2000\n",
      "36/36 [==============================] - 3s 95ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 2.5061 - val_accuracy: 0.6562 - lr: 1.2500e-04\n",
      "Epoch 228/2000\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.5091 - val_accuracy: 0.6632 - lr: 1.2500e-04\n",
      "Epoch 229/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0109 - accuracy: 0.9974 - val_loss: 2.5403 - val_accuracy: 0.6528 - lr: 1.2500e-04\n",
      "Epoch 230/2000\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.0070 - accuracy: 0.9965 - val_loss: 2.5695 - val_accuracy: 0.6562 - lr: 1.2500e-04\n",
      "Epoch 231/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 2.5828 - val_accuracy: 0.6493 - lr: 1.2500e-04\n",
      "Epoch 232/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0038 - accuracy: 0.9983 - val_loss: 2.5816 - val_accuracy: 0.6528 - lr: 1.2500e-04\n",
      "Epoch 233/2000\n",
      "36/36 [==============================] - 3s 96ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 2.5693 - val_accuracy: 0.6667 - lr: 1.2500e-04\n",
      "Epoch 234/2000\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 2.6323 - val_accuracy: 0.6562 - lr: 1.2500e-04\n",
      "Epoch 235/2000\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0040 - accuracy: 0.9983 - val_loss: 2.6575 - val_accuracy: 0.6667 - lr: 1.2500e-04\n",
      "Epoch 236/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.6550 - val_accuracy: 0.6528 - lr: 1.2500e-04\n",
      "Epoch 237/2000\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 2.6861 - val_accuracy: 0.6632 - lr: 1.2500e-04\n",
      "Epoch 238/2000\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.0034 - accuracy: 0.9983 - val_loss: 2.7337 - val_accuracy: 0.6632 - lr: 1.2500e-04\n",
      "Epoch 239/2000\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.7217 - val_accuracy: 0.6597 - lr: 1.2500e-04\n",
      "Epoch 240/2000\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 2.7358 - val_accuracy: 0.6632 - lr: 1.2500e-04\n",
      "Epoch 241/2000\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 0.0195 - accuracy: 0.9931 - val_loss: 2.8114 - val_accuracy: 0.6562 - lr: 1.2500e-04\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 1.5606 - accuracy: 0.5139\n",
      "Validation Loss: 1.5606, Validation Accuracy: 0.5139\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
