{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking model: lighteternal/fact-or-opinion-xlmr-el\n",
      "Loading model: lighteternal/fact-or-opinion-xlmr-el on cpu\n",
      "Run completed in 0.7401s (avg 0.1233s per sentence)\n",
      "Run completed in 0.5590s (avg 0.0932s per sentence)\n",
      "Run completed in 0.5560s (avg 0.0927s per sentence)\n",
      "\n",
      "Benchmarking model: GroNLP/mdebertav3-subjectivity-multilingual\n",
      "Loading model: GroNLP/mdebertav3-subjectivity-multilingual on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\miniconda3\\envs\\miaa\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andre\\.cache\\huggingface\\hub\\models--GroNLP--mdebertav3-subjectivity-multilingual. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed in 2.3878s (avg 0.3980s per sentence)\n",
      "Run completed in 2.4740s (avg 0.4123s per sentence)\n",
      "Run completed in 2.4978s (avg 0.4163s per sentence)\n",
      "\n",
      "Results for lighteternal/fact-or-opinion-xlmr-el:\n",
      "                                                text  objective_score  \\\n",
      "0                 The Earth revolves around the Sun.         0.001928   \n",
      "1         I think this movie is absolutely terrible.         0.997180   \n",
      "2   Water boils at 100 degrees Celsius at sea level.         0.003098   \n",
      "3  In my opinion, the government should increase ...         0.994117   \n",
      "4                    Paris is the capital of France.         0.001870   \n",
      "5      This is probably the best restaurant in town.         0.013378   \n",
      "\n",
      "   subjective_score classification  inference_time  \n",
      "0          0.998072        OPINION        0.199014  \n",
      "1          0.002820           FACT        0.125007  \n",
      "2          0.996902        OPINION        0.122011  \n",
      "3          0.005883           FACT        0.114010  \n",
      "4          0.998130        OPINION        0.086004  \n",
      "5          0.986622        OPINION        0.094007  \n",
      "\n",
      "Results for GroNLP/mdebertav3-subjectivity-multilingual:\n",
      "                                                 text  objective_score  \\\n",
      "6                  The Earth revolves around the Sun.         0.947622   \n",
      "7          I think this movie is absolutely terrible.         0.023000   \n",
      "8    Water boils at 100 degrees Celsius at sea level.         0.989563   \n",
      "9   In my opinion, the government should increase ...         0.074654   \n",
      "10                    Paris is the capital of France.         0.981899   \n",
      "11      This is probably the best restaurant in town.         0.021860   \n",
      "\n",
      "    subjective_score classification  inference_time  \n",
      "6           0.052378           FACT        0.349900  \n",
      "7           0.977000        OPINION        0.372080  \n",
      "8           0.010437           FACT        0.380607  \n",
      "9           0.925346        OPINION        0.459381  \n",
      "10          0.018101           FACT        0.470325  \n",
      "11          0.978140        OPINION        0.354459  \n",
      "\n",
      "Performance Summary:\n",
      "                                         model  avg_inference_time\n",
      "1         lighteternal/fact-or-opinion-xlmr-el            0.103063\n",
      "0  GroNLP/mdebertav3-subjectivity-multilingual            0.408807\n",
      "\n",
      "Detailed results saved to 'subjectivity_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "class SubjectivityClassifier:\n",
    "    def __init__(self, model_name, device=None):\n",
    "        \"\"\"\n",
    "        Initialize a subjectivity classifier with a specified pre-trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained model to use\n",
    "            device (str, optional): Device to run the model on ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Loading model: {model_name} on {self.device}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def classify(self, text, return_time=False):\n",
    "        \"\"\"\n",
    "        Classify text as subjective (opinion) or objective (fact).\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to classify\n",
    "            return_time (bool): Whether to return inference time\n",
    "            \n",
    "        Returns:\n",
    "            dict: Classification results including scores and inference time if requested\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Check if the model has 2 or more output classes\n",
    "        if probs.shape[1] >= 2:\n",
    "            # Assuming: 0 = objective/fact, 1 = subjective/opinion\n",
    "            # This is the common convention but may need adjustment for specific models\n",
    "            obj_score = probs[0][0].item()\n",
    "            subj_score = probs[0][1].item()\n",
    "            classification = \"FACT\" if obj_score > subj_score else \"OPINION\"\n",
    "        else:\n",
    "            # For single score models (rare)\n",
    "            subj_score = probs[0][0].item()\n",
    "            obj_score = 1 - subj_score\n",
    "            classification = \"OPINION\" if subj_score >= 0.5 else \"FACT\"\n",
    "        \n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        \n",
    "        result = {\n",
    "            \"text\": text,\n",
    "            \"objective_score\": obj_score,\n",
    "            \"subjective_score\": subj_score,\n",
    "            \"classification\": classification\n",
    "        }\n",
    "        \n",
    "        if return_time:\n",
    "            result[\"inference_time\"] = inference_time\n",
    "            \n",
    "        return result\n",
    "\n",
    "def benchmark_models(models, test_sentences, runs=3):\n",
    "    \"\"\"\n",
    "    Benchmark multiple subjectivity classification models.\n",
    "    \n",
    "    Args:\n",
    "        models (list): List of model names to benchmark\n",
    "        test_sentences (list): List of sentences to test\n",
    "        runs (int): Number of runs for more reliable timing\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Benchmark results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(f\"\\nBenchmarking model: {model_name}\")\n",
    "        classifier = SubjectivityClassifier(model_name)\n",
    "        \n",
    "        # Warmup run\n",
    "        for sentence in test_sentences:\n",
    "            classifier.classify(sentence)\n",
    "        \n",
    "        # Benchmark runs\n",
    "        model_times = []\n",
    "        results = []\n",
    "        \n",
    "        for _ in range(runs):\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            for sentence in test_sentences:\n",
    "                result = classifier.classify(sentence, return_time=True)\n",
    "                if _ == 0:  # Only save classification results from first run\n",
    "                    results.append(result)\n",
    "                model_times.append(result[\"inference_time\"])\n",
    "            \n",
    "            batch_end = time.time()\n",
    "            batch_time = batch_end - batch_start\n",
    "            print(f\"Run completed in {batch_time:.4f}s (avg {batch_time/len(test_sentences):.4f}s per sentence)\")\n",
    "        \n",
    "        # Calculate average inference time\n",
    "        avg_time = np.mean(model_times)\n",
    "        \n",
    "        # Add model name and timing information to results\n",
    "        for result in results:\n",
    "            result[\"model\"] = model_name\n",
    "            result[\"avg_inference_time\"] = avg_time\n",
    "            all_results.append(result)\n",
    "    \n",
    "    # Create and return DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Models to benchmark (selected specifically for subjectivity classification)\n",
    "    models = [\n",
    "        #\"prithivida/parrot_subjectivity_classifier\",  # Popular subjectivity classifier\n",
    "        #\"textattack/distilbert-base-uncased-MPQA\",    # DistilBERT fine-tuned on MPQA\n",
    "        #\"yuriykatko/subjectivity-classifier\"          # Custom subjectivity classifier\n",
    "\n",
    "        \"lighteternal/fact-or-opinion-xlmr-el\",\n",
    "        \"GroNLP/mdebertav3-subjectivity-multilingual\"\n",
    "    ]\n",
    "    \n",
    "    # Test sentences (mix of clear facts and opinions)\n",
    "    test_sentences = [\n",
    "        \"The Earth revolves around the Sun.\",\n",
    "        \"I think this movie is absolutely terrible.\",\n",
    "        \"Water boils at 100 degrees Celsius at sea level.\",\n",
    "        \"In my opinion, the government should increase funding for education.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"This is probably the best restaurant in town.\"\n",
    "    ]\n",
    "    \n",
    "    # Run benchmark\n",
    "    results_df = benchmark_models(models, test_sentences)\n",
    "    \n",
    "    # Display results by model\n",
    "    for model in models:\n",
    "        print(f\"\\nResults for {model}:\")\n",
    "        model_df = results_df[results_df[\"model\"] == model]\n",
    "        print(model_df[[\"text\", \"objective_score\", \"subjective_score\", \"classification\", \"inference_time\"]])\n",
    "    \n",
    "    # Display performance summary\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    performance_df = results_df.groupby(\"model\")[\"avg_inference_time\"].mean().reset_index()\n",
    "    performance_df = performance_df.sort_values(\"avg_inference_time\")\n",
    "    print(performance_df)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(\"subjectivity_benchmark_results.csv\", index=False)\n",
    "    print(\"\\nDetailed results saved to 'subjectivity_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>objective_score</th>\n",
       "      <th>subjective_score</th>\n",
       "      <th>classification</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>model</th>\n",
       "      <th>avg_inference_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I think this movie is absolutely terrible.</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>OPINION</td>\n",
       "      <td>0.372080</td>\n",
       "      <td>GroNLP/mdebertav3-subjectivity-multilingual</td>\n",
       "      <td>0.408807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Water boils at 100 degrees Celsius at sea level.</td>\n",
       "      <td>0.989563</td>\n",
       "      <td>0.010437</td>\n",
       "      <td>FACT</td>\n",
       "      <td>0.380607</td>\n",
       "      <td>GroNLP/mdebertav3-subjectivity-multilingual</td>\n",
       "      <td>0.408807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In my opinion, the government should increase ...</td>\n",
       "      <td>0.074654</td>\n",
       "      <td>0.925346</td>\n",
       "      <td>OPINION</td>\n",
       "      <td>0.459381</td>\n",
       "      <td>GroNLP/mdebertav3-subjectivity-multilingual</td>\n",
       "      <td>0.408807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Paris is the capital of France.</td>\n",
       "      <td>0.981899</td>\n",
       "      <td>0.018101</td>\n",
       "      <td>FACT</td>\n",
       "      <td>0.470325</td>\n",
       "      <td>GroNLP/mdebertav3-subjectivity-multilingual</td>\n",
       "      <td>0.408807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This is probably the best restaurant in town.</td>\n",
       "      <td>0.021860</td>\n",
       "      <td>0.978140</td>\n",
       "      <td>OPINION</td>\n",
       "      <td>0.354459</td>\n",
       "      <td>GroNLP/mdebertav3-subjectivity-multilingual</td>\n",
       "      <td>0.408807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  objective_score  \\\n",
       "7          I think this movie is absolutely terrible.         0.023000   \n",
       "8    Water boils at 100 degrees Celsius at sea level.         0.989563   \n",
       "9   In my opinion, the government should increase ...         0.074654   \n",
       "10                    Paris is the capital of France.         0.981899   \n",
       "11      This is probably the best restaurant in town.         0.021860   \n",
       "\n",
       "    subjective_score classification  inference_time  \\\n",
       "7           0.977000        OPINION        0.372080   \n",
       "8           0.010437           FACT        0.380607   \n",
       "9           0.925346        OPINION        0.459381   \n",
       "10          0.018101           FACT        0.470325   \n",
       "11          0.978140        OPINION        0.354459   \n",
       "\n",
       "                                          model  avg_inference_time  \n",
       "7   GroNLP/mdebertav3-subjectivity-multilingual            0.408807  \n",
       "8   GroNLP/mdebertav3-subjectivity-multilingual            0.408807  \n",
       "9   GroNLP/mdebertav3-subjectivity-multilingual            0.408807  \n",
       "10  GroNLP/mdebertav3-subjectivity-multilingual            0.408807  \n",
       "11  GroNLP/mdebertav3-subjectivity-multilingual            0.408807  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
