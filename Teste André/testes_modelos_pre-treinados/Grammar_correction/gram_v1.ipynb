{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Gramformer, T5 and torch for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\miniconda3\\envs\\miaa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from gramformer import Gramformer\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize T5 Transformer (Gramformer Base model) using torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedT5Corrector:\n",
    "    def __init__(self):\n",
    "        # Load model and tokenizer directly\n",
    "        self.model_name = \"prithivida/grammar_error_correcter_v1\"\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Optimize model for inference\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Use torch.compile for PyTorch 2.0+ (significant speedup)\n",
    "        if hasattr(torch, 'compile'):\n",
    "            try:\n",
    "                self.model = torch.compile(self.model)\n",
    "                print(\"Successfully applied torch.compile optimization\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not apply torch.compile: {e}\")\n",
    "        \n",
    "        # Optimize memory usage\n",
    "        self.model.config.use_cache = True\n",
    "        \n",
    "    def correct(self, sentence, max_length=128):\n",
    "        # Apply inference optimizations\n",
    "        with torch.inference_mode():\n",
    "            # Prepare input - the \"gec:\" prefix is important for the model\n",
    "            input_text = f\"gec: {sentence}\"\n",
    "            input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "            \n",
    "            # Optimize generation parameters for speed\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=max_length,\n",
    "                num_beams=2,  #2 # Reduced from 5 for speed\n",
    "                early_stopping=True,\n",
    "                use_cache=True  # Enable KV caching for faster generation\n",
    "            )\n",
    "            \n",
    "            # Decode output\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing original Gramformer...\n",
      "[Gramformer] Grammar error correct/highlight model loaded..\n",
      "Original time: 9.1652 seconds\n",
      "Corrected: The coffee was spilled over de console of my brand new Porche. This car is very special to me and this shall not be happening again.\n",
      "\n",
      "Testing optimized implementation...\n",
      "Successfully applied torch.compile optimization\n",
      "Optimized time: 5.1128 seconds\n",
      "Corrected: The coffee was spilled over de console of my brand new Porche. This car is very special to me and this shall not be happening again.\n",
      "\n",
      "Speedup: 1.79x\n"
     ]
    }
   ],
   "source": [
    "incorrect = \"He have been working on this project for three year.\"\n",
    "incorrect = \"Me don tthink that this new helicopter is going to be helping the fire crew combact the fire\"\n",
    "incorrect = \"The coffe was spilled ar over de console of my brand new Porche. This car is very special to me and this shall not be hapening again\"\n",
    "    \n",
    "# Test with Gramformer\n",
    "print(\"Testing original Gramformer...\")\n",
    "start = time.time()\n",
    "gf = Gramformer(models=1, use_gpu=False)\n",
    "corrected = list(gf.correct(incorrect, max_candidates=1))[0]\n",
    "original_time = time.time() - start\n",
    "print(f\"Original time: {original_time:.4f} seconds\")\n",
    "print(f\"Corrected: {corrected}\")\n",
    "\n",
    "# Test with optimized implementation\n",
    "print(\"\\nTesting optimized implementation...\")\n",
    "start = time.time()\n",
    "corrector = OptimizedT5Corrector()\n",
    "corrected = corrector.correct(incorrect)\n",
    "optimized_time = time.time() - start\n",
    "print(f\"Optimized time: {optimized_time:.4f} seconds\")\n",
    "print(f\"Corrected: {corrected}\")\n",
    "\n",
    "# Calculate speedup\n",
    "if optimized_time > 0:\n",
    "    speedup = original_time / optimized_time\n",
    "    print(f\"\\nSpeedup: {speedup:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
