{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script tests an optimization approach using torch.inference_mode() but the results seem to be worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n",
      "Standard approach: 5.2601 seconds\n",
      "Corrected: He has been working on this project for three years.\n",
      "[Gramformer] Grammar error correct/highlight model loaded..\n",
      "\n",
      "Optimized approach: 6.9656 seconds\n",
      "Corrected: He has been working on this project for three years.\n",
      "\n",
      "Improvement: -32.42%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from gramformer import Gramformer\n",
    "\n",
    "def optimize_inference(sentence):\n",
    "    # Initialize Gramformer (do this only once in your application)\n",
    "    gf = Gramformer(models=1, use_gpu=False)\n",
    "    \n",
    "    # Use inference mode for faster processing\n",
    "    with torch.inference_mode():\n",
    "        # Get all corrections\n",
    "        corrected_sentences = gf.correct(sentence, max_candidates=1)\n",
    "        \n",
    "        # Return the first correction\n",
    "        for corrected_sentence in corrected_sentences:\n",
    "            return corrected_sentence\n",
    "    \n",
    "    # If no corrections found, return the original sentence\n",
    "    return sentence\n",
    "\n",
    "# Test function\n",
    "def compare_performance():\n",
    "    incorrect = \"He have been working on this project for three year.\"\n",
    "    \n",
    "    # Standard approach\n",
    "    start_time = time.time()\n",
    "    gf = Gramformer(models=1, use_gpu=False)\n",
    "    corrected = list(gf.correct(incorrect, max_candidates=1))[0]\n",
    "    standard_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Standard approach: {standard_time:.4f} seconds\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    \n",
    "    # Optimized approach \n",
    "    start_time = time.time()\n",
    "    corrected = optimize_inference(incorrect)\n",
    "    optimized_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nOptimized approach: {optimized_time:.4f} seconds\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if optimized_time > 0:\n",
    "        improvement = (standard_time - optimized_time) / standard_time * 100\n",
    "        print(f\"\\nImprovement: {improvement:.2f}%\")\n",
    "\n",
    "# Run the comparison\n",
    "compare_performance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
