{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization approach here was quantization. It reduces the model size but it significantly increased the inference time for some unknown reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regular Gramformer...\n",
      "[Gramformer] Grammar error correct/highlight model loaded..\n",
      "Regular time: 5.2717 seconds\n",
      "Corrected: He has been working on this project for three years.\n",
      "\n",
      "Testing optimized Gramformer...\n",
      "[Gramformer] Grammar error correct/highlight model loaded..\n",
      "Could not apply TorchScript optimization: Comprehension ifs are not supported yet:\n",
      "  File \"c:\\Users\\andre\\miniconda3\\envs\\miaa\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\", line 1179\n",
      "    \n",
      "        if not return_dict:\n",
      "            return tuple(\n",
      "                v\n",
      "                for v in [\n",
      "\n",
      "Successfully applied quantization\n",
      "Optimized time: 15.8964 seconds\n",
      "Corrected: He has been working on this project for three years.\n",
      "\n",
      "Speedup: 0.33x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gramformer import Gramformer\n",
    "import time\n",
    "\n",
    "def create_optimized_gramformer():\n",
    "    # Initialize regular Gramformer\n",
    "    gf = Gramformer(models=1, use_gpu=False)\n",
    "    \n",
    "    # Get the underlying model\n",
    "    model = gf.correction_model\n",
    "    \n",
    "    # Optimize the model\n",
    "    # 1. Set to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Apply torch.jit optimization\n",
    "    try:\n",
    "        # Create dummy input for tracing\n",
    "        dummy_text = \"This is a sample sentence with error.\"\n",
    "        dummy_input = gf.correction_tokenizer(f\"gec: {dummy_text}\", \n",
    "                                           return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        # Trace the model\n",
    "        with torch.inference_mode():\n",
    "            # Use script instead of trace for better optimization\n",
    "            traced_model = torch.jit.script(model)\n",
    "            \n",
    "        # Replace the original model\n",
    "        gf.correction_model = traced_model\n",
    "        \n",
    "        print(\"Successfully optimized the model with TorchScript\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not apply TorchScript optimization: {e}\")\n",
    "    \n",
    "    # 3. Apply quantization\n",
    "    try:\n",
    "        # Quantize the model to int8\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            model, \n",
    "            {torch.nn.Linear}, \n",
    "            dtype=torch.qint8\n",
    "        )\n",
    "        \n",
    "        # Replace the original model\n",
    "        gf.correction_model = quantized_model\n",
    "        \n",
    "        print(\"Successfully applied quantization\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not apply quantization: {e}\")\n",
    "    \n",
    "    return gf\n",
    "\n",
    "# Test function\n",
    "def test_optimized_gramformer():\n",
    "    incorrect = \"He have been working on this project for three year.\"\n",
    "    \n",
    "    # Test regular Gramformer\n",
    "    print(\"Testing regular Gramformer...\")\n",
    "    start_time = time.time()\n",
    "    regular_gf = Gramformer(models=1, use_gpu=False)\n",
    "    corrected = list(regular_gf.correct(incorrect, max_candidates=1))[0]\n",
    "    regular_time = time.time() - start_time\n",
    "    print(f\"Regular time: {regular_time:.4f} seconds\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    \n",
    "    # Test optimized Gramformer\n",
    "    print(\"\\nTesting optimized Gramformer...\")\n",
    "    start_time = time.time()\n",
    "    optimized_gf = create_optimized_gramformer()\n",
    "    corrected = list(optimized_gf.correct(incorrect, max_candidates=1))[0]\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized time: {optimized_time:.4f} seconds\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    \n",
    "    # Show speedup\n",
    "    speedup = regular_time / optimized_time\n",
    "    print(f\"\\nSpeedup: {speedup:.2f}x\")\n",
    "\n",
    "# Run the test\n",
    "test_optimized_gramformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
