{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 376 images for class 'angry'.\n",
      "Found 192 images for class 'disgust'.\n",
      "Found 376 images for class 'fear'.\n",
      "Found 376 images for class 'happy'.\n",
      "Found 376 images for class 'neutral'.\n",
      "Found 376 images for class 'sad'.\n",
      "Found 192 images for class 'surprise'.\n",
      "Original dataset: 2264 samples\n",
      "Augmented dataset: 3624 samples\n",
      "Train: 2536, Validation: 544, Test: 544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RNN</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,114,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RNN</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn (\u001b[38;5;33mRNN\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m1,114,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_1 (\u001b[38;5;33mRNN\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m455\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,188,103</span> (4.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,188,103\u001b[0m (4.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,187,783</span> (4.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,187,783\u001b[0m (4.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> (1.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m320\u001b[0m (1.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.1310 - loss: 2.5878 - precision: 0.1613 - recall: 0.0378\n",
      "Epoch 1: val_loss improved from inf to 2.12892, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 340ms/step - accuracy: 0.1311 - loss: 2.5867 - precision: 0.1613 - recall: 0.0377 - val_accuracy: 0.1673 - val_loss: 2.1289 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.1550 - loss: 2.1788 - precision: 0.1124 - recall: 0.0040\n",
      "Epoch 2: val_loss improved from 2.12892 to 2.10699, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 349ms/step - accuracy: 0.1550 - loss: 2.1787 - precision: 0.1124 - recall: 0.0040 - val_accuracy: 0.1673 - val_loss: 2.1070 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 0.1401 - loss: 2.1533 - precision: 0.1984 - recall: 0.0029\n",
      "Epoch 3: val_loss improved from 2.10699 to 2.01600, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 350ms/step - accuracy: 0.1401 - loss: 2.1530 - precision: 0.1989 - recall: 0.0029 - val_accuracy: 0.1673 - val_loss: 2.0160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 0.1860 - loss: 2.0785 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4: val_loss improved from 2.01600 to 1.99265, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 373ms/step - accuracy: 0.1860 - loss: 2.0785 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.1618 - val_loss: 1.9927 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 0.1563 - loss: 2.0515 - precision: 0.1960 - recall: 3.1539e-04\n",
      "Epoch 5: val_loss improved from 1.99265 to 1.96451, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 374ms/step - accuracy: 0.1564 - loss: 2.0514 - precision: 0.1964 - recall: 3.1589e-04 - val_accuracy: 0.1893 - val_loss: 1.9645 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.2052 - loss: 1.9932 - precision: 0.2944 - recall: 0.0017\n",
      "Epoch 6: val_loss did not improve from 1.96451\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 369ms/step - accuracy: 0.2052 - loss: 1.9934 - precision: 0.2944 - recall: 0.0017 - val_accuracy: 0.1691 - val_loss: 2.0483 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step - accuracy: 0.2020 - loss: 1.9845 - precision: 0.3312 - recall: 0.0057\n",
      "Epoch 7: val_loss improved from 1.96451 to 1.81000, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 380ms/step - accuracy: 0.2021 - loss: 1.9842 - precision: 0.3316 - recall: 0.0057 - val_accuracy: 0.2757 - val_loss: 1.8100 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 0.2414 - loss: 1.8447 - precision: 0.3162 - recall: 0.0093\n",
      "Epoch 8: val_loss did not improve from 1.81000\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 385ms/step - accuracy: 0.2415 - loss: 1.8445 - precision: 0.3173 - recall: 0.0094 - val_accuracy: 0.2849 - val_loss: 1.9284 - val_precision: 0.6667 - val_recall: 0.0110 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - accuracy: 0.2841 - loss: 1.7743 - precision: 0.4296 - recall: 0.0335\n",
      "Epoch 9: val_loss did not improve from 1.81000\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 354ms/step - accuracy: 0.2841 - loss: 1.7742 - precision: 0.4300 - recall: 0.0336 - val_accuracy: 0.2960 - val_loss: 1.9423 - val_precision: 0.5833 - val_recall: 0.0257 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.3357 - loss: 1.6754 - precision: 0.5433 - recall: 0.0760\n",
      "Epoch 10: val_loss improved from 1.81000 to 1.65875, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 344ms/step - accuracy: 0.3356 - loss: 1.6755 - precision: 0.5432 - recall: 0.0759 - val_accuracy: 0.3621 - val_loss: 1.6587 - val_precision: 0.5529 - val_recall: 0.0864 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 0.3726 - loss: 1.6101 - precision: 0.5982 - recall: 0.0976\n",
      "Epoch 11: val_loss improved from 1.65875 to 1.53247, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 357ms/step - accuracy: 0.3726 - loss: 1.6102 - precision: 0.5980 - recall: 0.0976 - val_accuracy: 0.3603 - val_loss: 1.5325 - val_precision: 0.5269 - val_recall: 0.0901 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.3832 - loss: 1.5830 - precision: 0.6019 - recall: 0.0859\n",
      "Epoch 12: val_loss did not improve from 1.53247\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 358ms/step - accuracy: 0.3831 - loss: 1.5831 - precision: 0.6017 - recall: 0.0859 - val_accuracy: 0.1691 - val_loss: 3.6973 - val_precision: 0.1777 - val_recall: 0.1673 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.3829 - loss: 1.5647 - precision: 0.5375 - recall: 0.1076\n",
      "Epoch 13: val_loss did not improve from 1.53247\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 346ms/step - accuracy: 0.3829 - loss: 1.5645 - precision: 0.5377 - recall: 0.1077 - val_accuracy: 0.3842 - val_loss: 1.7166 - val_precision: 0.4874 - val_recall: 0.1783 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.4166 - loss: 1.5249 - precision: 0.6111 - recall: 0.1311\n",
      "Epoch 14: val_loss improved from 1.53247 to 1.44419, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 326ms/step - accuracy: 0.4166 - loss: 1.5249 - precision: 0.6111 - recall: 0.1311 - val_accuracy: 0.4669 - val_loss: 1.4442 - val_precision: 0.6467 - val_recall: 0.2188 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.4663 - loss: 1.4346 - precision: 0.6147 - recall: 0.1640\n",
      "Epoch 15: val_loss did not improve from 1.44419\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 323ms/step - accuracy: 0.4663 - loss: 1.4344 - precision: 0.6148 - recall: 0.1642 - val_accuracy: 0.4761 - val_loss: 1.4816 - val_precision: 0.5404 - val_recall: 0.2831 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.4742 - loss: 1.3817 - precision: 0.6699 - recall: 0.2362\n",
      "Epoch 16: val_loss improved from 1.44419 to 1.38408, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 322ms/step - accuracy: 0.4742 - loss: 1.3818 - precision: 0.6698 - recall: 0.2362 - val_accuracy: 0.4945 - val_loss: 1.3841 - val_precision: 0.6189 - val_recall: 0.3015 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.4853 - loss: 1.3729 - precision: 0.6335 - recall: 0.2424\n",
      "Epoch 17: val_loss did not improve from 1.38408\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 317ms/step - accuracy: 0.4854 - loss: 1.3728 - precision: 0.6336 - recall: 0.2425 - val_accuracy: 0.3842 - val_loss: 1.8683 - val_precision: 0.4738 - val_recall: 0.3162 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.5455 - loss: 1.2781 - precision: 0.7084 - recall: 0.3211\n",
      "Epoch 18: val_loss did not improve from 1.38408\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 320ms/step - accuracy: 0.5455 - loss: 1.2781 - precision: 0.7083 - recall: 0.3212 - val_accuracy: 0.3217 - val_loss: 2.1927 - val_precision: 0.3733 - val_recall: 0.2463 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.5571 - loss: 1.2299 - precision: 0.6837 - recall: 0.3595\n",
      "Epoch 19: val_loss improved from 1.38408 to 1.29521, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 324ms/step - accuracy: 0.5571 - loss: 1.2299 - precision: 0.6837 - recall: 0.3595 - val_accuracy: 0.5055 - val_loss: 1.2952 - val_precision: 0.6034 - val_recall: 0.3860 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.6056 - loss: 1.1418 - precision: 0.7102 - recall: 0.3878\n",
      "Epoch 20: val_loss did not improve from 1.29521\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 332ms/step - accuracy: 0.6055 - loss: 1.1420 - precision: 0.7101 - recall: 0.3879 - val_accuracy: 0.5147 - val_loss: 1.3132 - val_precision: 0.5836 - val_recall: 0.3787 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.6271 - loss: 1.0718 - precision: 0.7361 - recall: 0.4546\n",
      "Epoch 21: val_loss did not improve from 1.29521\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 324ms/step - accuracy: 0.6271 - loss: 1.0720 - precision: 0.7359 - recall: 0.4545 - val_accuracy: 0.4596 - val_loss: 1.7089 - val_precision: 0.5073 - val_recall: 0.3842 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.6480 - loss: 1.0361 - precision: 0.7507 - recall: 0.5044\n",
      "Epoch 22: val_loss improved from 1.29521 to 1.21012, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 324ms/step - accuracy: 0.6480 - loss: 1.0360 - precision: 0.7507 - recall: 0.5044 - val_accuracy: 0.5919 - val_loss: 1.2101 - val_precision: 0.6520 - val_recall: 0.5165 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.6779 - loss: 0.9459 - precision: 0.7740 - recall: 0.5809\n",
      "Epoch 23: val_loss did not improve from 1.21012\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 327ms/step - accuracy: 0.6779 - loss: 0.9458 - precision: 0.7740 - recall: 0.5810 - val_accuracy: 0.4504 - val_loss: 1.8313 - val_precision: 0.4810 - val_recall: 0.4191 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.7167 - loss: 0.8465 - precision: 0.8004 - recall: 0.6388\n",
      "Epoch 24: val_loss did not improve from 1.21012\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 324ms/step - accuracy: 0.7167 - loss: 0.8467 - precision: 0.8003 - recall: 0.6388 - val_accuracy: 0.5515 - val_loss: 1.3228 - val_precision: 0.6063 - val_recall: 0.4926 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.7142 - loss: 0.8614 - precision: 0.7886 - recall: 0.6408\n",
      "Epoch 25: val_loss did not improve from 1.21012\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 342ms/step - accuracy: 0.7143 - loss: 0.8611 - precision: 0.7887 - recall: 0.6409 - val_accuracy: 0.5790 - val_loss: 1.2162 - val_precision: 0.6312 - val_recall: 0.5349 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.7738 - loss: 0.6915 - precision: 0.8358 - recall: 0.7198\n",
      "Epoch 26: val_loss improved from 1.21012 to 1.04007, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 338ms/step - accuracy: 0.7738 - loss: 0.6915 - precision: 0.8357 - recall: 0.7198 - val_accuracy: 0.6691 - val_loss: 1.0401 - val_precision: 0.7071 - val_recall: 0.6213 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.7891 - loss: 0.6992 - precision: 0.8257 - recall: 0.7325\n",
      "Epoch 27: val_loss improved from 1.04007 to 0.85801, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 329ms/step - accuracy: 0.7891 - loss: 0.6994 - precision: 0.8257 - recall: 0.7325 - val_accuracy: 0.7224 - val_loss: 0.8580 - val_precision: 0.7707 - val_recall: 0.6857 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.8180 - loss: 0.5827 - precision: 0.8647 - recall: 0.7780\n",
      "Epoch 28: val_loss did not improve from 0.85801\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 333ms/step - accuracy: 0.8179 - loss: 0.5829 - precision: 0.8646 - recall: 0.7779 - val_accuracy: 0.5625 - val_loss: 1.6223 - val_precision: 0.5813 - val_recall: 0.5386 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.8012 - loss: 0.6431 - precision: 0.8449 - recall: 0.7635\n",
      "Epoch 29: val_loss did not improve from 0.85801\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 334ms/step - accuracy: 0.8013 - loss: 0.6430 - precision: 0.8449 - recall: 0.7635 - val_accuracy: 0.6066 - val_loss: 1.5404 - val_precision: 0.6275 - val_recall: 0.5790 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.8221 - loss: 0.5726 - precision: 0.8511 - recall: 0.7915\n",
      "Epoch 30: val_loss did not improve from 0.85801\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 323ms/step - accuracy: 0.8222 - loss: 0.5725 - precision: 0.8511 - recall: 0.7916 - val_accuracy: 0.7169 - val_loss: 1.0121 - val_precision: 0.7350 - val_recall: 0.6985 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.8579 - loss: 0.5010 - precision: 0.8867 - recall: 0.8360\n",
      "Epoch 31: val_loss did not improve from 0.85801\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 323ms/step - accuracy: 0.8579 - loss: 0.5012 - precision: 0.8867 - recall: 0.8359 - val_accuracy: 0.7390 - val_loss: 0.9275 - val_precision: 0.7538 - val_recall: 0.7261 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.8700 - loss: 0.4538 - precision: 0.8882 - recall: 0.8494\n",
      "Epoch 32: val_loss improved from 0.85801 to 0.83570, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 328ms/step - accuracy: 0.8700 - loss: 0.4539 - precision: 0.8882 - recall: 0.8494 - val_accuracy: 0.7721 - val_loss: 0.8357 - val_precision: 0.8015 - val_recall: 0.7647 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.8935 - loss: 0.3959 - precision: 0.9129 - recall: 0.8741\n",
      "Epoch 33: val_loss improved from 0.83570 to 0.79915, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 327ms/step - accuracy: 0.8934 - loss: 0.3962 - precision: 0.9128 - recall: 0.8741 - val_accuracy: 0.7831 - val_loss: 0.7991 - val_precision: 0.8175 - val_recall: 0.7739 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.8921 - loss: 0.4296 - precision: 0.9057 - recall: 0.8715\n",
      "Epoch 34: val_loss did not improve from 0.79915\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 332ms/step - accuracy: 0.8920 - loss: 0.4298 - precision: 0.9057 - recall: 0.8714 - val_accuracy: 0.7482 - val_loss: 0.9542 - val_precision: 0.7596 - val_recall: 0.7261 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.8800 - loss: 0.4490 - precision: 0.8978 - recall: 0.8669\n",
      "Epoch 35: val_loss improved from 0.79915 to 0.72315, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 330ms/step - accuracy: 0.8800 - loss: 0.4490 - precision: 0.8978 - recall: 0.8669 - val_accuracy: 0.7941 - val_loss: 0.7232 - val_precision: 0.8038 - val_recall: 0.7757 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.9212 - loss: 0.3431 - precision: 0.9296 - recall: 0.9061\n",
      "Epoch 36: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 353ms/step - accuracy: 0.9212 - loss: 0.3432 - precision: 0.9296 - recall: 0.9060 - val_accuracy: 0.6232 - val_loss: 1.5965 - val_precision: 0.6374 - val_recall: 0.6140 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - accuracy: 0.9053 - loss: 0.3636 - precision: 0.9185 - recall: 0.8950\n",
      "Epoch 37: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 362ms/step - accuracy: 0.9054 - loss: 0.3635 - precision: 0.9185 - recall: 0.8951 - val_accuracy: 0.7923 - val_loss: 0.8889 - val_precision: 0.8092 - val_recall: 0.7794 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - accuracy: 0.9350 - loss: 0.2987 - precision: 0.9438 - recall: 0.9189\n",
      "Epoch 38: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 382ms/step - accuracy: 0.9349 - loss: 0.2988 - precision: 0.9438 - recall: 0.9189 - val_accuracy: 0.7996 - val_loss: 0.8396 - val_precision: 0.8090 - val_recall: 0.7941 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.9271 - loss: 0.2970 - precision: 0.9430 - recall: 0.9207\n",
      "Epoch 39: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 336ms/step - accuracy: 0.9271 - loss: 0.2970 - precision: 0.9430 - recall: 0.9207 - val_accuracy: 0.7537 - val_loss: 1.1128 - val_precision: 0.7710 - val_recall: 0.7426 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.9224 - loss: 0.3184 - precision: 0.9309 - recall: 0.9153\n",
      "Epoch 40: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 338ms/step - accuracy: 0.9224 - loss: 0.3183 - precision: 0.9309 - recall: 0.9153 - val_accuracy: 0.4908 - val_loss: 2.5839 - val_precision: 0.5000 - val_recall: 0.4853 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.9123 - loss: 0.3470 - precision: 0.9223 - recall: 0.9065\n",
      "Epoch 41: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 331ms/step - accuracy: 0.9123 - loss: 0.3470 - precision: 0.9223 - recall: 0.9065 - val_accuracy: 0.7445 - val_loss: 1.1993 - val_precision: 0.7567 - val_recall: 0.7316 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.9450 - loss: 0.2691 - precision: 0.9497 - recall: 0.9349\n",
      "Epoch 42: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 342ms/step - accuracy: 0.9449 - loss: 0.2693 - precision: 0.9497 - recall: 0.9348 - val_accuracy: 0.7831 - val_loss: 0.8976 - val_precision: 0.7996 - val_recall: 0.7702 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - accuracy: 0.9437 - loss: 0.2666 - precision: 0.9511 - recall: 0.9353\n",
      "Epoch 43: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 361ms/step - accuracy: 0.9437 - loss: 0.2668 - precision: 0.9511 - recall: 0.9352 - val_accuracy: 0.8180 - val_loss: 0.7843 - val_precision: 0.8340 - val_recall: 0.8125 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.9351 - loss: 0.2645 - precision: 0.9455 - recall: 0.9293\n",
      "Epoch 44: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 348ms/step - accuracy: 0.9351 - loss: 0.2645 - precision: 0.9455 - recall: 0.9293 - val_accuracy: 0.8033 - val_loss: 0.8658 - val_precision: 0.8112 - val_recall: 0.7978 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.9475 - loss: 0.2116 - precision: 0.9559 - recall: 0.9417\n",
      "Epoch 45: val_loss did not improve from 0.72315\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 341ms/step - accuracy: 0.9475 - loss: 0.2117 - precision: 0.9559 - recall: 0.9417 - val_accuracy: 0.7739 - val_loss: 1.2046 - val_precision: 0.7813 - val_recall: 0.7684 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.9431 - loss: 0.2529 - precision: 0.9491 - recall: 0.9394\n",
      "Epoch 46: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 346ms/step - accuracy: 0.9432 - loss: 0.2527 - precision: 0.9492 - recall: 0.9395 - val_accuracy: 0.8254 - val_loss: 0.7922 - val_precision: 0.8386 - val_recall: 0.8217 - learning_rate: 5.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.9743 - loss: 0.1692 - precision: 0.9773 - recall: 0.9707\n",
      "Epoch 47: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 342ms/step - accuracy: 0.9744 - loss: 0.1691 - precision: 0.9773 - recall: 0.9707 - val_accuracy: 0.8309 - val_loss: 0.8246 - val_precision: 0.8433 - val_recall: 0.8309 - learning_rate: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.9850 - loss: 0.1260 - precision: 0.9871 - recall: 0.9835\n",
      "Epoch 48: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 336ms/step - accuracy: 0.9849 - loss: 0.1259 - precision: 0.9871 - recall: 0.9835 - val_accuracy: 0.8346 - val_loss: 0.7701 - val_precision: 0.8401 - val_recall: 0.8309 - learning_rate: 5.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.9827 - loss: 0.1164 - precision: 0.9853 - recall: 0.9825\n",
      "Epoch 49: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 341ms/step - accuracy: 0.9827 - loss: 0.1164 - precision: 0.9853 - recall: 0.9825 - val_accuracy: 0.8419 - val_loss: 0.7910 - val_precision: 0.8447 - val_recall: 0.8401 - learning_rate: 5.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.9853 - loss: 0.1232 - precision: 0.9862 - recall: 0.9841\n",
      "Epoch 50: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 340ms/step - accuracy: 0.9853 - loss: 0.1232 - precision: 0.9862 - recall: 0.9841 - val_accuracy: 0.8162 - val_loss: 1.0196 - val_precision: 0.8170 - val_recall: 0.8125 - learning_rate: 5.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.9883 - loss: 0.1124 - precision: 0.9884 - recall: 0.9860\n",
      "Epoch 51: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 323ms/step - accuracy: 0.9883 - loss: 0.1124 - precision: 0.9884 - recall: 0.9859 - val_accuracy: 0.8309 - val_loss: 1.0305 - val_precision: 0.8321 - val_recall: 0.8290 - learning_rate: 5.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.9815 - loss: 0.1385 - precision: 0.9831 - recall: 0.9782\n",
      "Epoch 52: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 324ms/step - accuracy: 0.9815 - loss: 0.1385 - precision: 0.9831 - recall: 0.9782 - val_accuracy: 0.8438 - val_loss: 0.8221 - val_precision: 0.8476 - val_recall: 0.8382 - learning_rate: 5.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.9863 - loss: 0.1135 - precision: 0.9864 - recall: 0.9856\n",
      "Epoch 53: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 322ms/step - accuracy: 0.9863 - loss: 0.1135 - precision: 0.9864 - recall: 0.9856 - val_accuracy: 0.8346 - val_loss: 0.9234 - val_precision: 0.8423 - val_recall: 0.8346 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.9918 - loss: 0.0954 - precision: 0.9918 - recall: 0.9880\n",
      "Epoch 54: val_loss did not improve from 0.72315\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 323ms/step - accuracy: 0.9918 - loss: 0.0955 - precision: 0.9918 - recall: 0.9880 - val_accuracy: 0.7960 - val_loss: 1.1596 - val_precision: 0.7985 - val_recall: 0.7941 - learning_rate: 5.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.9819 - loss: 0.1334 - precision: 0.9842 - recall: 0.9796\n",
      "Epoch 55: val_loss did not improve from 0.72315\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 339ms/step - accuracy: 0.9818 - loss: 0.1335 - precision: 0.9841 - recall: 0.9796 - val_accuracy: 0.8290 - val_loss: 0.9053 - val_precision: 0.8321 - val_recall: 0.8290 - learning_rate: 5.0000e-04\n",
      "Epoch 55: early stopping\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7646 - loss: 0.8558 - precision: 0.7910 - recall: 0.7545\n",
      "Test Loss: 0.8435\n",
      "Test Accuracy: 0.7812\n",
      "Test Precision: 0.8019\n",
      "Test Recall: 0.7665\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.72      0.89      0.79        89\n",
      "     disgust       0.88      0.67      0.76        45\n",
      "        fear       0.79      0.75      0.77        91\n",
      "       happy       0.80      0.74      0.77        90\n",
      "     neutral       0.88      0.78      0.83        91\n",
      "         sad       0.71      0.75      0.73        93\n",
      "    surprise       0.80      0.89      0.84        45\n",
      "\n",
      "    accuracy                           0.78       544\n",
      "   macro avg       0.80      0.78      0.78       544\n",
      "weighted avg       0.79      0.78      0.78       544\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, RNN, LSTMCell, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def load_data(dataset_path, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Loads precomputed mel-spectrogram images and extracts labels from folder names.\n",
    "    Returns both raw data and data split into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(dataset_path))  # Get emotion categories\n",
    "    \n",
    "    for label in class_names:\n",
    "        class_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        files = glob.glob(os.path.join(class_path, '*.png'))\n",
    "        print(f\"Found {len(files)} images for class '{label}'.\")\n",
    "        \n",
    "        for file in files:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "            img = cv2.resize(img, img_size)  # Resize to standard size\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "    y = np.array(y)\n",
    "    return X, y, class_names\n",
    "\n",
    "def augment_data(X, y, augmentation_factor=0.3):\n",
    "    \"\"\"Apply simple data augmentation to improve model generalization\"\"\"\n",
    "    aug_X = []\n",
    "    aug_y = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        # Original sample\n",
    "        aug_X.append(X[i])\n",
    "        aug_y.append(y[i])\n",
    "        \n",
    "        # Apply time stretching (horizontal scaling)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            stretched = cv2.resize(X[i], (int(X[i].shape[1] * 1.1), X[i].shape[0]))\n",
    "            stretched = cv2.resize(stretched, (X[i].shape[1], X[i].shape[0]))\n",
    "            aug_X.append(stretched)\n",
    "            aug_y.append(y[i])\n",
    "        \n",
    "        # Apply frequency masking (random horizontal lines masked)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            masked = X[i].copy()\n",
    "            num_masks = np.random.randint(1, 4)\n",
    "            for _ in range(num_masks):\n",
    "                freq_width = np.random.randint(5, 15)\n",
    "                freq_start = np.random.randint(0, X[i].shape[0] - freq_width)\n",
    "                masked[freq_start:freq_start+freq_width, :] = 0\n",
    "            aug_X.append(masked)\n",
    "            aug_y.append(y[i])\n",
    "    \n",
    "    return np.array(aug_X), np.array(aug_y)\n",
    "\n",
    "def preprocess_labels(y):\n",
    "    \"\"\"Encodes string labels into numerical one-hot vectors.\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "    return y_onehot, le\n",
    "\n",
    "def build_hybrid_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a hybrid CNN-LSTM model that leverages both spatial and temporal features.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First, add CNN layers to extract spatial features\n",
    "    model.add(Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Reshape for LSTM - treat rows as time steps\n",
    "    model.add(Reshape((32, 32*64)))\n",
    "    \n",
    "    # LSTM layers for temporal features\n",
    "    model.add(RNN(LSTMCell(128), return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(RNN(LSTMCell(64)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Classification layers\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Configuration parameters\n",
    "    dataset_path = 'Ravdess_mel_specs'  # Update with your dataset path\n",
    "    img_size = (128, 128)\n",
    "    test_size = 0.15\n",
    "    val_size = 0.15\n",
    "    batch_size = 16\n",
    "    max_epochs = 100\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y, class_names = load_data(dataset_path, img_size)\n",
    "    print(f\"Original dataset: {X.shape[0]} samples\")\n",
    "    \n",
    "    # Apply data augmentation\n",
    "    X_aug, y_aug = augment_data(X, y)\n",
    "    print(f\"Augmented dataset: {X_aug.shape[0]} samples\")\n",
    "    \n",
    "    # Encode labels\n",
    "    y_onehot, le = preprocess_labels(y_aug)\n",
    "    \n",
    "    # Create train, validation, and test sets\n",
    "    # First split off the test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_aug, y_onehot, test_size=test_size, random_state=42, \n",
    "        stratify=np.argmax(y_onehot, axis=1)\n",
    "    )\n",
    "    \n",
    "    # Then split the remaining data into train and validation\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, random_state=42,\n",
    "        stratify=np.argmax(y_temp, axis=1)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]}, Validation: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Build and compile the model\n",
    "    input_shape = X_train.shape[1:]  # (128, 128)\n",
    "    num_classes = y_onehot.shape[1]\n",
    "    model = build_hybrid_model(input_shape, num_classes)\n",
    "    \n",
    "    # Use a learning rate schedule with warmup\n",
    "    initial_lr = 0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks for training\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_emotion_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stop, checkpoint, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    # Generate predictions and evaluate in detail\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true_classes, \n",
    "        y_pred_classes, \n",
    "        target_names=[le.inverse_transform([i])[0] for i in range(num_classes)]\n",
    "    ))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VERIFICACIÓN DE GPU =====\n",
      "\n",
      "1. Verificación con TensorFlow:\n",
      "   ✕ TensorFlow no detectó ninguna GPU\n",
      "\n",
      "2. Variables de entorno:\n",
      "   CUDA_VISIBLE_DEVICES = 0\n",
      "\n",
      "3. Verificación con nvidia-smi:\n",
      "   ✓ nvidia-smi detectó GPU:\n",
      "     Sat Mar 15 00:43:05 2025       \n",
      "     +-----------------------------------------------------------------------------------------+\n",
      "     | NVIDIA-SMI 555.99                 Driver Version: 555.99         CUDA Version: 12.5     |\n",
      "     |-----------------------------------------+------------------------+----------------------+\n",
      "     | GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "     | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "     |                                         |                        |               MIG M. |\n",
      "     |=========================================+========================+======================|\n",
      "     |   0  NVIDIA GeForce RTX 3060 Ti   WDDM  |   00000000:0B:00.0  On |                  N/A |\n",
      "     |  0%   52C    P0             14W /  200W |    1692MiB /   8192MiB |      8%      Default |\n",
      "     ...\n",
      "\n",
      "4. Verificación de CUDA en TensorFlow:\n",
      "   Compilado con CUDA: False\n",
      "   Dispositivos GPU disponibles: False\n",
      "\n",
      "5. Prueba de operación en GPU:\n",
      "   ✓ Operación exitosa en GPU: Resultado de forma (2, 2)\n",
      "   Dispositivo usado: /job:localhost/replica:0/task:0/device:CPU:0\n",
      "\n",
      "==============================\n",
      "Verificando disponibilidad de GPU con TensorFlow:\n",
      "ADVERTENCIA: TensorFlow no detectó ninguna GPU.\n",
      "Comprobando con información del sistema:\n",
      "Información de GPU según nvidia-smi:\n",
      "Sat Mar 15 00:43:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.99                 Driver Version: 555.99         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  |   00000000:0B:00.0  On |                  N/A |\n",
      "|  0%   52C    P0             14W /  200W |    1692MiB /   8192MiB |      8%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2580    C+G   ...ft Office\\root\\Office16\\WINWORD.EXE      N/A      |\n",
      "|    0   N/A  N/A      7068    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     10588    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     10652    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10796    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11504    C+G   ...on\\133.0.3065.92\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     13348    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     13428    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13532    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13924    C+G   ..._x64__cw5n1h2txyewy\\WidgetBoard.exe      N/A      |\n",
      "|    0   N/A  N/A     15740    C+G   ...Data\\Local\\Programs\\Opera\\opera.exe      N/A      |\n",
      "|    0   N/A  N/A     16848    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17876    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19532    C+G   ...on\\133.0.3065.92\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     20068    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     21728    C+G   ...Data\\Local\\Programs\\Opera\\opera.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Por favor verifica que TensorFlow esté correctamente instalado con soporte CUDA.\n",
      "\n",
      "Intentando forzar el uso de GPU mediante variables de entorno...\n",
      "Aún no se detectan GPUs. El entrenamiento usará CPU y será significativamente más lento.\n",
      "ADVERTENCIA: No se encontró GPU. El entrenamiento será más lento.\n",
      "Found 376 images for class 'angry'.\n",
      "Found 192 images for class 'disgust'.\n",
      "Found 376 images for class 'fear'.\n",
      "Found 376 images for class 'happy'.\n",
      "Found 376 images for class 'neutral'.\n",
      "Found 376 images for class 'sad'.\n",
      "Found 192 images for class 'surprise'.\n",
      "Original dataset: 2264 samples\n",
      "Augmented dataset: 3624 samples\n",
      "Train: 2536, Validation: 544, Test: 544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RNN</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,114,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RNN</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_2 (\u001b[38;5;33mRNN\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m1,114,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_3 (\u001b[38;5;33mRNN\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m455\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,188,103</span> (4.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,188,103\u001b[0m (4.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,187,783</span> (4.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,187,783\u001b[0m (4.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> (1.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m320\u001b[0m (1.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estado de GPU (Época 0):\n",
      "  GPU 0: Utilización 22%, Memoria: 1612/8192 MB\n",
      "Epoch 1/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1530 - loss: 2.6532 - precision_1: 0.1396 - recall_1: 0.0443\n",
      "Epoch 1: val_loss improved from inf to 2.04342, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo de época: 52.16s, Promedio últimas 1 épocas: 52.16s\n",
      "Tiempo estimado restante: 5164.09s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.1530 - loss: 2.6507 - precision_1: 0.1395 - recall_1: 0.0441 - val_accuracy: 0.1746 - val_loss: 2.0434 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1569 - loss: 2.3367 - precision_1: 0.1891 - recall_1: 0.0240\n",
      "Epoch 2: val_loss did not improve from 2.04342\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.1573 - loss: 2.3350 - precision_1: 0.1894 - recall_1: 0.0239 - val_accuracy: 0.1636 - val_loss: 2.0528 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 2):\n",
      "  GPU 0: Utilización 11%, Memoria: 1559/8192 MB\n",
      "Epoch 3/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2224 - loss: 2.1359 - precision_1: 0.2835 - recall_1: 0.0316\n",
      "Epoch 3: val_loss did not improve from 2.04342\n",
      "\n",
      "Tiempo de época: 53.53s, Promedio últimas 2 épocas: 52.01s\n",
      "Tiempo estimado restante: 5044.63s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.2225 - loss: 2.1352 - precision_1: 0.2845 - recall_1: 0.0317 - val_accuracy: 0.1636 - val_loss: 2.1321 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2034 - loss: 2.0902 - precision_1: 0.3151 - recall_1: 0.0340\n",
      "Epoch 4: val_loss did not improve from 2.04342\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.2038 - loss: 2.0890 - precision_1: 0.3167 - recall_1: 0.0342 - val_accuracy: 0.1636 - val_loss: 2.1784 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 4):\n",
      "  GPU 0: Utilización 29%, Memoria: 1566/8192 MB\n",
      "Epoch 5/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2983 - loss: 1.8735 - precision_1: 0.4209 - recall_1: 0.0706\n",
      "Epoch 5: val_loss did not improve from 2.04342\n",
      "\n",
      "Tiempo de época: 52.92s, Promedio últimas 2 épocas: 52.84s\n",
      "Tiempo estimado restante: 5019.34s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.2983 - loss: 1.8731 - precision_1: 0.4218 - recall_1: 0.0708 - val_accuracy: 0.1048 - val_loss: 2.1748 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3231 - loss: 1.7826 - precision_1: 0.5648 - recall_1: 0.1098\n",
      "Epoch 6: val_loss did not improve from 2.04342\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.3233 - loss: 1.7818 - precision_1: 0.5654 - recall_1: 0.1100 - val_accuracy: 0.0882 - val_loss: 2.1915 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 6):\n",
      "  GPU 0: Utilización 63%, Memoria: 1596/8192 MB\n",
      "Epoch 7/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3586 - loss: 1.6936 - precision_1: 0.5614 - recall_1: 0.1302\n",
      "Epoch 7: val_loss did not improve from 2.04342\n",
      "\n",
      "Tiempo de época: 52.04s, Promedio últimas 2 épocas: 51.62s\n",
      "Tiempo estimado restante: 4800.55s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.3584 - loss: 1.6941 - precision_1: 0.5614 - recall_1: 0.1301 - val_accuracy: 0.0901 - val_loss: 2.3491 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3504 - loss: 1.6857 - precision_1: 0.5113 - recall_1: 0.1187\n",
      "Epoch 8: val_loss did not improve from 2.04342\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.3506 - loss: 1.6855 - precision_1: 0.5124 - recall_1: 0.1191 - val_accuracy: 0.1066 - val_loss: 2.4294 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 8):\n",
      "  GPU 0: Utilización 11%, Memoria: 1576/8192 MB\n",
      "Epoch 9/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3906 - loss: 1.5677 - precision_1: 0.5726 - recall_1: 0.1622\n",
      "Epoch 9: val_loss improved from 2.04342 to 2.03517, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo de época: 46.50s, Promedio últimas 2 épocas: 48.31s\n",
      "Tiempo estimado restante: 4396.36s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.3908 - loss: 1.5674 - precision_1: 0.5730 - recall_1: 0.1621 - val_accuracy: 0.1949 - val_loss: 2.0352 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4267 - loss: 1.5315 - precision_1: 0.6041 - recall_1: 0.1763\n",
      "Epoch 10: val_loss did not improve from 2.03517\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.4270 - loss: 1.5307 - precision_1: 0.6045 - recall_1: 0.1765 - val_accuracy: 0.2004 - val_loss: 2.0990 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 10):\n",
      "  GPU 0: Utilización 8%, Memoria: 1571/8192 MB\n",
      "Epoch 11/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4662 - loss: 1.4138 - precision_1: 0.6510 - recall_1: 0.2231\n",
      "Epoch 11: val_loss did not improve from 2.03517\n",
      "\n",
      "Tiempo de época: 46.49s, Promedio últimas 2 épocas: 46.26s\n",
      "Tiempo estimado restante: 4116.91s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.4663 - loss: 1.4138 - precision_1: 0.6509 - recall_1: 0.2231 - val_accuracy: 0.1930 - val_loss: 2.2471 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5018 - loss: 1.3223 - precision_1: 0.7199 - recall_1: 0.2766\n",
      "Epoch 12: val_loss improved from 2.03517 to 1.82641, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.5019 - loss: 1.3220 - precision_1: 0.7193 - recall_1: 0.2768 - val_accuracy: 0.3217 - val_loss: 1.8264 - val_precision_1: 0.4340 - val_recall_1: 0.0846 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 12):\n",
      "  GPU 0: Utilización 10%, Memoria: 1595/8192 MB\n",
      "Epoch 13/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5517 - loss: 1.2207 - precision_1: 0.6870 - recall_1: 0.3074\n",
      "Epoch 13: val_loss did not improve from 1.82641\n",
      "\n",
      "Tiempo de época: 45.68s, Promedio últimas 2 épocas: 47.44s\n",
      "Tiempo estimado restante: 4127.06s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.5517 - loss: 1.2209 - precision_1: 0.6869 - recall_1: 0.3075 - val_accuracy: 0.2353 - val_loss: 2.1370 - val_precision_1: 0.2805 - val_recall_1: 0.1268 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5609 - loss: 1.1869 - precision_1: 0.7094 - recall_1: 0.3538\n",
      "Epoch 14: val_loss did not improve from 1.82641\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.5612 - loss: 1.1862 - precision_1: 0.7096 - recall_1: 0.3541 - val_accuracy: 0.3125 - val_loss: 2.2612 - val_precision_1: 0.3592 - val_recall_1: 0.2555 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 14):\n",
      "  GPU 0: Utilización 10%, Memoria: 1595/8192 MB\n",
      "Epoch 15/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6018 - loss: 1.1044 - precision_1: 0.7365 - recall_1: 0.4270\n",
      "Epoch 15: val_loss did not improve from 1.82641\n",
      "\n",
      "Tiempo de época: 46.09s, Promedio últimas 2 épocas: 45.44s\n",
      "Tiempo estimado restante: 3862.59s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.6017 - loss: 1.1049 - precision_1: 0.7362 - recall_1: 0.4268 - val_accuracy: 0.2188 - val_loss: 3.1052 - val_precision_1: 0.2186 - val_recall_1: 0.1949 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6165 - loss: 1.0613 - precision_1: 0.7351 - recall_1: 0.4360\n",
      "Epoch 16: val_loss did not improve from 1.82641\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.6169 - loss: 1.0605 - precision_1: 0.7353 - recall_1: 0.4366 - val_accuracy: 0.2040 - val_loss: 3.5264 - val_precision_1: 0.2188 - val_recall_1: 0.1967 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 16):\n",
      "  GPU 0: Utilización 11%, Memoria: 1597/8192 MB\n",
      "Epoch 17/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6463 - loss: 0.9839 - precision_1: 0.7576 - recall_1: 0.5060\n",
      "Epoch 17: val_loss improved from 1.82641 to 1.40436, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo de época: 46.05s, Promedio últimas 2 épocas: 45.66s\n",
      "Tiempo estimado restante: 3790.01s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.6467 - loss: 0.9835 - precision_1: 0.7575 - recall_1: 0.5063 - val_accuracy: 0.5184 - val_loss: 1.4044 - val_precision_1: 0.5737 - val_recall_1: 0.4651 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7080 - loss: 0.8747 - precision_1: 0.7946 - recall_1: 0.5747\n",
      "Epoch 18: val_loss improved from 1.40436 to 1.24149, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.7077 - loss: 0.8751 - precision_1: 0.7941 - recall_1: 0.5748 - val_accuracy: 0.5901 - val_loss: 1.2415 - val_precision_1: 0.6286 - val_recall_1: 0.5165 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 18):\n",
      "  GPU 0: Utilización 14%, Memoria: 1606/8192 MB\n",
      "Epoch 19/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7499 - loss: 0.7545 - precision_1: 0.8100 - recall_1: 0.6460\n",
      "Epoch 19: val_loss improved from 1.24149 to 0.95391, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo de época: 44.76s, Promedio últimas 2 épocas: 46.15s\n",
      "Tiempo estimado restante: 3738.17s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.7498 - loss: 0.7553 - precision_1: 0.8100 - recall_1: 0.6460 - val_accuracy: 0.6636 - val_loss: 0.9539 - val_precision_1: 0.7075 - val_recall_1: 0.6048 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7507 - loss: 0.7635 - precision_1: 0.8159 - recall_1: 0.6852\n",
      "Epoch 20: val_loss improved from 0.95391 to 0.91303, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.7504 - loss: 0.7640 - precision_1: 0.8156 - recall_1: 0.6848 - val_accuracy: 0.7243 - val_loss: 0.9130 - val_precision_1: 0.7548 - val_recall_1: 0.6562 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 20):\n",
      "  GPU 0: Utilización 20%, Memoria: 1598/8192 MB\n",
      "Epoch 21/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7791 - loss: 0.7057 - precision_1: 0.8355 - recall_1: 0.7051\n",
      "Epoch 21: val_loss did not improve from 0.91303\n",
      "\n",
      "Tiempo de época: 44.74s, Promedio últimas 2 épocas: 44.65s\n",
      "Tiempo estimado restante: 3527.31s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.7792 - loss: 0.7056 - precision_1: 0.8356 - recall_1: 0.7051 - val_accuracy: 0.5460 - val_loss: 1.6750 - val_precision_1: 0.5726 - val_recall_1: 0.5147 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8197 - loss: 0.6169 - precision_1: 0.8607 - recall_1: 0.7695\n",
      "Epoch 22: val_loss did not improve from 0.91303\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.8198 - loss: 0.6164 - precision_1: 0.8607 - recall_1: 0.7697 - val_accuracy: 0.7004 - val_loss: 1.0544 - val_precision_1: 0.7273 - val_recall_1: 0.6765 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 22):\n",
      "  GPU 0: Utilización 11%, Memoria: 1604/8192 MB\n",
      "Epoch 23/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8544 - loss: 0.5314 - precision_1: 0.8867 - recall_1: 0.8124\n",
      "Epoch 23: val_loss improved from 0.91303 to 0.90043, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo de época: 44.75s, Promedio últimas 2 épocas: 44.71s\n",
      "Tiempo estimado restante: 3443.05s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.8543 - loss: 0.5313 - precision_1: 0.8865 - recall_1: 0.8123 - val_accuracy: 0.7335 - val_loss: 0.9004 - val_precision_1: 0.7495 - val_recall_1: 0.7096 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8387 - loss: 0.5531 - precision_1: 0.8804 - recall_1: 0.7946\n",
      "Epoch 24: val_loss did not improve from 0.90043\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.8387 - loss: 0.5530 - precision_1: 0.8802 - recall_1: 0.7947 - val_accuracy: 0.6967 - val_loss: 1.0198 - val_precision_1: 0.7235 - val_recall_1: 0.6783 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 24):\n",
      "  GPU 0: Utilización 11%, Memoria: 1597/8192 MB\n",
      "Epoch 25/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8710 - loss: 0.4602 - precision_1: 0.8888 - recall_1: 0.8434\n",
      "Epoch 25: val_loss did not improve from 0.90043\n",
      "\n",
      "Tiempo de época: 44.30s, Promedio últimas 2 épocas: 44.41s\n",
      "Tiempo estimado restante: 3330.90s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.8712 - loss: 0.4597 - precision_1: 0.8890 - recall_1: 0.8437 - val_accuracy: 0.6985 - val_loss: 1.1342 - val_precision_1: 0.7148 - val_recall_1: 0.6912 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9109 - loss: 0.3958 - precision_1: 0.9285 - recall_1: 0.8849\n",
      "Epoch 26: val_loss did not improve from 0.90043\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.9109 - loss: 0.3955 - precision_1: 0.9284 - recall_1: 0.8849 - val_accuracy: 0.7188 - val_loss: 1.1383 - val_precision_1: 0.7327 - val_recall_1: 0.7004 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 26):\n",
      "  GPU 0: Utilización 11%, Memoria: 1604/8192 MB\n",
      "Epoch 27/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9104 - loss: 0.3885 - precision_1: 0.9228 - recall_1: 0.8921\n",
      "Epoch 27: val_loss did not improve from 0.90043\n",
      "\n",
      "Tiempo de época: 44.48s, Promedio últimas 2 épocas: 44.40s\n",
      "Tiempo estimado restante: 3241.32s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.9104 - loss: 0.3882 - precision_1: 0.9228 - recall_1: 0.8921 - val_accuracy: 0.7537 - val_loss: 0.9394 - val_precision_1: 0.7584 - val_recall_1: 0.7445 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9107 - loss: 0.3483 - precision_1: 0.9288 - recall_1: 0.8977\n",
      "Epoch 28: val_loss did not improve from 0.90043\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.9107 - loss: 0.3482 - precision_1: 0.9287 - recall_1: 0.8976 - val_accuracy: 0.7261 - val_loss: 1.1671 - val_precision_1: 0.7428 - val_recall_1: 0.7114 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 28):\n",
      "  GPU 0: Utilización 2%, Memoria: 1644/8192 MB\n",
      "Epoch 29/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55s/step - accuracy: 0.9442 - loss: 0.2931 - precision_1: 0.9510 - recall_1: 0.9261 \n",
      "Epoch 29: val_loss did not improve from 0.90043\n",
      "\n",
      "Tiempo de época: 2152.94s, Promedio últimas 2 épocas: 1098.97s\n",
      "Tiempo estimado restante: 78026.57s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2153s\u001b[0m 55s/step - accuracy: 0.9437 - loss: 0.2940 - precision_1: 0.9506 - recall_1: 0.9256 - val_accuracy: 0.6562 - val_loss: 1.4869 - val_precision_1: 0.6764 - val_recall_1: 0.6379 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9079 - loss: 0.3650 - precision_1: 0.9168 - recall_1: 0.8891\n",
      "Epoch 30: val_loss did not improve from 0.90043\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.9080 - loss: 0.3646 - precision_1: 0.9169 - recall_1: 0.8892 - val_accuracy: 0.7426 - val_loss: 0.9442 - val_precision_1: 0.7709 - val_recall_1: 0.7298 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 30):\n",
      "  GPU 0: Utilización 3%, Memoria: 1274/8192 MB\n",
      "Epoch 31/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9113 - loss: 0.3252 - precision_1: 0.9222 - recall_1: 0.9022\n",
      "Epoch 31: val_loss improved from 0.90043 to 0.89106, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo de época: 44.58s, Promedio últimas 2 épocas: 45.22s\n",
      "Tiempo estimado restante: 3119.93s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.9115 - loss: 0.3249 - precision_1: 0.9224 - recall_1: 0.9024 - val_accuracy: 0.7721 - val_loss: 0.8911 - val_precision_1: 0.7876 - val_recall_1: 0.7702 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9395 - loss: 0.2743 - precision_1: 0.9517 - recall_1: 0.9366\n",
      "Epoch 32: val_loss improved from 0.89106 to 0.85443, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.9393 - loss: 0.2749 - precision_1: 0.9515 - recall_1: 0.9363 - val_accuracy: 0.7886 - val_loss: 0.8544 - val_precision_1: 0.8076 - val_recall_1: 0.7794 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 32):\n",
      "  GPU 0: Utilización 3%, Memoria: 1273/8192 MB\n",
      "Epoch 33/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9461 - loss: 0.2554 - precision_1: 0.9548 - recall_1: 0.9358\n",
      "Epoch 33: val_loss did not improve from 0.85443\n",
      "\n",
      "Tiempo de época: 47.31s, Promedio últimas 2 épocas: 46.03s\n",
      "Tiempo estimado restante: 3083.78s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.9460 - loss: 0.2554 - precision_1: 0.9547 - recall_1: 0.9358 - val_accuracy: 0.7518 - val_loss: 1.1040 - val_precision_1: 0.7624 - val_recall_1: 0.7371 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9520 - loss: 0.2151 - precision_1: 0.9601 - recall_1: 0.9457\n",
      "Epoch 34: val_loss did not improve from 0.85443\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.9520 - loss: 0.2152 - precision_1: 0.9601 - recall_1: 0.9458 - val_accuracy: 0.7555 - val_loss: 1.1424 - val_precision_1: 0.7631 - val_recall_1: 0.7518 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 34):\n",
      "  GPU 0: Utilización 0%, Memoria: 1468/8192 MB\n",
      "Epoch 35/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9715 - loss: 0.1815 - precision_1: 0.9759 - recall_1: 0.9664\n",
      "Epoch 35: val_loss did not improve from 0.85443\n",
      "\n",
      "Tiempo de época: 50.83s, Promedio últimas 2 épocas: 48.85s\n",
      "Tiempo estimado restante: 3175.44s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.9714 - loss: 0.1816 - precision_1: 0.9758 - recall_1: 0.9664 - val_accuracy: 0.7941 - val_loss: 1.0472 - val_precision_1: 0.7981 - val_recall_1: 0.7849 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9652 - loss: 0.1877 - precision_1: 0.9710 - recall_1: 0.9620\n",
      "Epoch 36: val_loss did not improve from 0.85443\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.9650 - loss: 0.1881 - precision_1: 0.9708 - recall_1: 0.9618 - val_accuracy: 0.7904 - val_loss: 0.9711 - val_precision_1: 0.7981 - val_recall_1: 0.7849 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 36):\n",
      "  GPU 0: Utilización 22%, Memoria: 1393/8192 MB\n",
      "Epoch 37/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9634 - loss: 0.1872 - precision_1: 0.9671 - recall_1: 0.9606\n",
      "Epoch 37: val_loss did not improve from 0.85443\n",
      "\n",
      "Tiempo de época: 46.55s, Promedio últimas 2 épocas: 48.11s\n",
      "Tiempo estimado restante: 3031.18s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.9632 - loss: 0.1878 - precision_1: 0.9669 - recall_1: 0.9604 - val_accuracy: 0.7151 - val_loss: 1.1945 - val_precision_1: 0.7341 - val_recall_1: 0.7004 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9638 - loss: 0.2172 - precision_1: 0.9664 - recall_1: 0.9578\n",
      "Epoch 38: val_loss did not improve from 0.85443\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.9637 - loss: 0.2170 - precision_1: 0.9664 - recall_1: 0.9578 - val_accuracy: 0.7831 - val_loss: 0.9553 - val_precision_1: 0.7951 - val_recall_1: 0.7702 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 38):\n",
      "  GPU 0: Utilización 10%, Memoria: 1405/8192 MB\n",
      "Epoch 39/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9721 - loss: 0.1892 - precision_1: 0.9759 - recall_1: 0.9692\n",
      "Epoch 39: val_loss did not improve from 0.85443\n",
      "\n",
      "Tiempo de época: 45.84s, Promedio últimas 2 épocas: 45.63s\n",
      "Tiempo estimado restante: 2783.69s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.9721 - loss: 0.1890 - precision_1: 0.9759 - recall_1: 0.9691 - val_accuracy: 0.8051 - val_loss: 0.9281 - val_precision_1: 0.8120 - val_recall_1: 0.7941 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9610 - loss: 0.1906 - precision_1: 0.9633 - recall_1: 0.9569\n",
      "Epoch 40: val_loss did not improve from 0.85443\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.9609 - loss: 0.1908 - precision_1: 0.9632 - recall_1: 0.9567 - val_accuracy: 0.7996 - val_loss: 0.9039 - val_precision_1: 0.8075 - val_recall_1: 0.7941 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 40):\n",
      "  GPU 0: Utilización 4%, Memoria: 1412/8192 MB\n",
      "Epoch 41/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9695 - loss: 0.1779 - precision_1: 0.9754 - recall_1: 0.9660\n",
      "Epoch 41: val_loss did not improve from 0.85443\n",
      "\n",
      "Tiempo de época: 45.92s, Promedio últimas 2 épocas: 45.44s\n",
      "Tiempo estimado restante: 2681.13s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.9694 - loss: 0.1782 - precision_1: 0.9753 - recall_1: 0.9659 - val_accuracy: 0.6820 - val_loss: 1.5869 - val_precision_1: 0.6906 - val_recall_1: 0.6728 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9600 - loss: 0.1945 - precision_1: 0.9651 - recall_1: 0.9551\n",
      "Epoch 42: val_loss did not improve from 0.85443\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.9601 - loss: 0.1943 - precision_1: 0.9652 - recall_1: 0.9552 - val_accuracy: 0.7629 - val_loss: 1.1634 - val_precision_1: 0.7732 - val_recall_1: 0.7518 - learning_rate: 0.0010\n",
      "\n",
      "Estado de GPU (Época 42):\n",
      "  GPU 0: Utilización 27%, Memoria: 1456/8192 MB\n",
      "Epoch 43/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9825 - loss: 0.1364 - precision_1: 0.9850 - recall_1: 0.9817\n",
      "Epoch 43: val_loss did not improve from 0.85443\n",
      "\n",
      "Tiempo de época: 48.83s, Promedio últimas 2 épocas: 47.74s\n",
      "Tiempo estimado restante: 2721.22s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.9826 - loss: 0.1362 - precision_1: 0.9851 - recall_1: 0.9818 - val_accuracy: 0.8088 - val_loss: 0.9041 - val_precision_1: 0.8175 - val_recall_1: 0.8070 - learning_rate: 5.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9898 - loss: 0.1178 - precision_1: 0.9902 - recall_1: 0.9874\n",
      "Epoch 44: val_loss did not improve from 0.85443\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9899 - loss: 0.1176 - precision_1: 0.9903 - recall_1: 0.9875 - val_accuracy: 0.8199 - val_loss: 0.8934 - val_precision_1: 0.8280 - val_recall_1: 0.8143 - learning_rate: 5.0000e-04\n",
      "\n",
      "Estado de GPU (Época 44):\n",
      "  GPU 0: Utilización 5%, Memoria: 1441/8192 MB\n",
      "Epoch 45/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9961 - loss: 0.1015 - precision_1: 0.9973 - recall_1: 0.9956\n",
      "Epoch 45: val_loss improved from 0.85443 to 0.77623, saving model to best_emotion_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiempo de época: 50.31s, Promedio últimas 2 épocas: 49.36s\n",
      "Tiempo estimado restante: 2714.94s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.9961 - loss: 0.1015 - precision_1: 0.9973 - recall_1: 0.9956 - val_accuracy: 0.8493 - val_loss: 0.7762 - val_precision_1: 0.8569 - val_recall_1: 0.8474 - learning_rate: 5.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0949 - precision_1: 0.9976 - recall_1: 0.9965\n",
      "Epoch 46: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0949 - precision_1: 0.9976 - recall_1: 0.9965 - val_accuracy: 0.8456 - val_loss: 0.8459 - val_precision_1: 0.8500 - val_recall_1: 0.8438 - learning_rate: 5.0000e-04\n",
      "\n",
      "Estado de GPU (Época 46):\n",
      "  GPU 0: Utilización 50%, Memoria: 1462/8192 MB\n",
      "Epoch 47/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0946 - precision_1: 0.9985 - recall_1: 0.9946\n",
      "Epoch 47: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 55.89s, Promedio últimas 2 épocas: 54.68s\n",
      "Tiempo estimado restante: 2898.04s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0945 - precision_1: 0.9985 - recall_1: 0.9946 - val_accuracy: 0.8382 - val_loss: 0.8358 - val_precision_1: 0.8395 - val_recall_1: 0.8364 - learning_rate: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9983 - loss: 0.0882 - precision_1: 0.9983 - recall_1: 0.9983\n",
      "Epoch 48: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.9983 - loss: 0.0882 - precision_1: 0.9983 - recall_1: 0.9983 - val_accuracy: 0.8419 - val_loss: 0.8300 - val_precision_1: 0.8476 - val_recall_1: 0.8382 - learning_rate: 5.0000e-04\n",
      "\n",
      "Estado de GPU (Época 48):\n",
      "  GPU 0: Utilización 31%, Memoria: 1491/8192 MB\n",
      "Epoch 49/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9986 - loss: 0.0860 - precision_1: 0.9999 - recall_1: 0.9970\n",
      "Epoch 49: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 56.61s, Promedio últimas 2 épocas: 55.28s\n",
      "Tiempo estimado restante: 2819.51s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.9986 - loss: 0.0860 - precision_1: 0.9998 - recall_1: 0.9970 - val_accuracy: 0.8566 - val_loss: 0.8238 - val_precision_1: 0.8577 - val_recall_1: 0.8529 - learning_rate: 5.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0840 - precision_1: 0.9976 - recall_1: 0.9975\n",
      "Epoch 50: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0840 - precision_1: 0.9976 - recall_1: 0.9975 - val_accuracy: 0.8493 - val_loss: 0.8359 - val_precision_1: 0.8506 - val_recall_1: 0.8474 - learning_rate: 5.0000e-04\n",
      "\n",
      "Estado de GPU (Época 50):\n",
      "  GPU 0: Utilización 11%, Memoria: 1532/8192 MB\n",
      "Epoch 51/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9954 - loss: 0.0898 - precision_1: 0.9960 - recall_1: 0.9954\n",
      "Epoch 51: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 52.72s, Promedio últimas 2 épocas: 54.34s\n",
      "Tiempo estimado restante: 2662.74s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.9954 - loss: 0.0898 - precision_1: 0.9960 - recall_1: 0.9954 - val_accuracy: 0.8548 - val_loss: 0.8635 - val_precision_1: 0.8587 - val_recall_1: 0.8493 - learning_rate: 5.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9991 - loss: 0.0822 - precision_1: 0.9991 - recall_1: 0.9991\n",
      "Epoch 52: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.9991 - loss: 0.0822 - precision_1: 0.9991 - recall_1: 0.9991 - val_accuracy: 0.8529 - val_loss: 0.8520 - val_precision_1: 0.8619 - val_recall_1: 0.8493 - learning_rate: 5.0000e-04\n",
      "\n",
      "Estado de GPU (Época 52):\n",
      "  GPU 0: Utilización 14%, Memoria: 1575/8192 MB\n",
      "Epoch 53/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9988 - loss: 0.0801 - precision_1: 0.9988 - recall_1: 0.9988\n",
      "Epoch 53: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 47.65s, Promedio últimas 2 épocas: 49.53s\n",
      "Tiempo estimado restante: 2327.73s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0801 - precision_1: 0.9987 - recall_1: 0.9987 - val_accuracy: 0.8382 - val_loss: 0.9608 - val_precision_1: 0.8395 - val_recall_1: 0.8364 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0808 - precision_1: 0.9970 - recall_1: 0.9968\n",
      "Epoch 54: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0807 - precision_1: 0.9970 - recall_1: 0.9968 - val_accuracy: 0.8456 - val_loss: 0.8426 - val_precision_1: 0.8532 - val_recall_1: 0.8438 - learning_rate: 5.0000e-04\n",
      "\n",
      "Estado de GPU (Época 54):\n",
      "  GPU 0: Utilización 13%, Memoria: 1515/8192 MB\n",
      "Epoch 55/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0786 - precision_1: 0.9987 - recall_1: 0.9987\n",
      "Epoch 55: val_loss did not improve from 0.77623\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Tiempo de época: 48.22s, Promedio últimas 2 épocas: 47.99s\n",
      "Tiempo estimado restante: 2159.74s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0785 - precision_1: 0.9987 - recall_1: 0.9987 - val_accuracy: 0.8401 - val_loss: 0.8726 - val_precision_1: 0.8416 - val_recall_1: 0.8401 - learning_rate: 5.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.0749 - precision_1: 0.9996 - recall_1: 0.9995\n",
      "Epoch 56: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.0749 - precision_1: 0.9996 - recall_1: 0.9994 - val_accuracy: 0.8290 - val_loss: 0.8838 - val_precision_1: 0.8343 - val_recall_1: 0.8235 - learning_rate: 2.5000e-04\n",
      "\n",
      "Estado de GPU (Época 56):\n",
      "  GPU 0: Utilización 29%, Memoria: 1556/8192 MB\n",
      "Epoch 57/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9998 - loss: 0.0747 - precision_1: 0.9998 - recall_1: 0.9998\n",
      "Epoch 57: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 48.91s, Promedio últimas 2 épocas: 48.90s\n",
      "Tiempo estimado restante: 2102.90s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.9998 - loss: 0.0747 - precision_1: 0.9998 - recall_1: 0.9998 - val_accuracy: 0.8474 - val_loss: 0.8435 - val_precision_1: 0.8516 - val_recall_1: 0.8438 - learning_rate: 2.5000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9988 - loss: 0.0746 - precision_1: 0.9988 - recall_1: 0.9988\n",
      "Epoch 58: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.9988 - loss: 0.0746 - precision_1: 0.9988 - recall_1: 0.9988 - val_accuracy: 0.8456 - val_loss: 0.8937 - val_precision_1: 0.8494 - val_recall_1: 0.8401 - learning_rate: 2.5000e-04\n",
      "\n",
      "Estado de GPU (Época 58):\n",
      "  GPU 0: Utilización 10%, Memoria: 1668/8192 MB\n",
      "Epoch 59/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9990 - loss: 0.0715 - precision_1: 0.9990 - recall_1: 0.9990\n",
      "Epoch 59: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 49.57s, Promedio últimas 2 épocas: 51.01s\n",
      "Tiempo estimado restante: 2091.58s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.9990 - loss: 0.0716 - precision_1: 0.9990 - recall_1: 0.9990 - val_accuracy: 0.8419 - val_loss: 0.9096 - val_precision_1: 0.8523 - val_recall_1: 0.8382 - learning_rate: 2.5000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0715 - precision_1: 1.0000 - recall_1: 1.0000\n",
      "Epoch 60: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0715 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.8456 - val_loss: 0.8786 - val_precision_1: 0.8513 - val_recall_1: 0.8419 - learning_rate: 2.5000e-04\n",
      "\n",
      "Estado de GPU (Época 60):\n",
      "  GPU 0: Utilización 9%, Memoria: 1806/8192 MB\n",
      "Epoch 61/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0707 - precision_1: 1.0000 - recall_1: 0.9995\n",
      "Epoch 61: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 49.28s, Promedio últimas 2 épocas: 50.77s\n",
      "Tiempo estimado restante: 1979.84s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0707 - precision_1: 1.0000 - recall_1: 0.9995 - val_accuracy: 0.8419 - val_loss: 0.8620 - val_precision_1: 0.8442 - val_recall_1: 0.8364 - learning_rate: 2.5000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 0.0748 - precision_1: 0.9979 - recall_1: 0.9979\n",
      "Epoch 62: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 0.0748 - precision_1: 0.9979 - recall_1: 0.9979 - val_accuracy: 0.8493 - val_loss: 0.8735 - val_precision_1: 0.8503 - val_recall_1: 0.8456 - learning_rate: 2.5000e-04\n",
      "\n",
      "Estado de GPU (Época 62):\n",
      "  GPU 0: Utilización 34%, Memoria: 1867/8192 MB\n",
      "Epoch 63/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0694 - precision_1: 1.0000 - recall_1: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.77623\n",
      "\n",
      "Tiempo de época: 51.14s, Promedio últimas 2 épocas: 51.31s\n",
      "Tiempo estimado restante: 1898.51s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0694 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.8456 - val_loss: 0.9033 - val_precision_1: 0.8456 - val_recall_1: 0.8456 - learning_rate: 2.5000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0685 - precision_1: 1.0000 - recall_1: 1.0000\n",
      "Epoch 64: val_loss did not improve from 0.77623\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0685 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.8401 - val_loss: 0.9253 - val_precision_1: 0.8429 - val_recall_1: 0.8382 - learning_rate: 2.5000e-04\n",
      "\n",
      "Estado de GPU (Época 64):\n",
      "  GPU 0: Utilización 20%, Memoria: 1689/8192 MB\n",
      "Epoch 65/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.0687 - precision_1: 0.9999 - recall_1: 0.9995\n",
      "Epoch 65: val_loss did not improve from 0.77623\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Tiempo de época: 50.04s, Promedio últimas 2 épocas: 49.90s\n",
      "Tiempo estimado restante: 1746.56s\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.0687 - precision_1: 0.9999 - recall_1: 0.9995 - val_accuracy: 0.8382 - val_loss: 0.9512 - val_precision_1: 0.8429 - val_recall_1: 0.8382 - learning_rate: 2.5000e-04\n",
      "Epoch 65: early stopping\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      "Tiempo total de entrenamiento: 1h 27m 38.50s\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - accuracy: 0.8675 - loss: 0.6928 - precision_1: 0.8695 - recall_1: 0.8647\n",
      "Test Loss: 0.7509\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 433\u001b[39m\n\u001b[32m    431\u001b[39m         f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModelo guardado en: best_emotion_model.h5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 384\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    381\u001b[39m test_metrics = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(model.metrics_names, test_results))\n\u001b[32m    383\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtest_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    385\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    386\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import psutil\n",
    "import datetime  # Para el registro de fecha/hora\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.layers import Dense, Dropout, RNN, LSTMCell, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "\n",
    "# Configurar el uso de GPU\n",
    "def configure_gpu():\n",
    "    \"\"\"Configura TensorFlow para utilizar GPU de forma explícita y óptima\"\"\"\n",
    "    # Verificar disponibilidad de GPU con TensorFlow\n",
    "    print(\"Verificando disponibilidad de GPU con TensorFlow:\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    \n",
    "    if gpus:\n",
    "        try:\n",
    "            # Configurar TensorFlow para usar todas las GPUs disponibles\n",
    "            print(f\"GPUs detectadas por TensorFlow: {len(gpus)}\")\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                print(f\"  GPU {i}: {gpu}\")\n",
    "                \n",
    "                # Permitir crecimiento de memoria según sea necesario\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "            # Configurar para usar la GPU como dispositivo predeterminado\n",
    "            tf.config.set_visible_devices(gpus, 'GPU')\n",
    "            \n",
    "            # Activar precisión mixta para acelerar el entrenamiento\n",
    "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "            print(\"Precisión mixta activada para acelerar entrenamiento\")\n",
    "            \n",
    "            # Verificar configuración\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"Se configuraron {len(logical_gpus)} GPUs lógicas\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error al configurar GPU: {e}\")\n",
    "    else:\n",
    "        print(\"ADVERTENCIA: TensorFlow no detectó ninguna GPU.\")\n",
    "        print(\"Comprobando con información del sistema:\")\n",
    "        \n",
    "        # Intentar obtener información del sistema\n",
    "        try:\n",
    "            import subprocess\n",
    "            gpu_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "            print(\"Información de GPU según nvidia-smi:\")\n",
    "            print(gpu_info)\n",
    "            print(\"\\nPor favor verifica que TensorFlow esté correctamente instalado con soporte CUDA.\")\n",
    "        except:\n",
    "            print(\"No se pudo ejecutar nvidia-smi. Es posible que los drivers de NVIDIA no estén instalados.\")\n",
    "        \n",
    "        # Intentar forzar el uso de GPU mediante variables de entorno\n",
    "        print(\"\\nIntentando forzar el uso de GPU mediante variables de entorno...\")\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "        os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "        \n",
    "        # Verificar de nuevo después de los ajustes\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            print(f\"Ahora se detectan {len(gpus)} GPUs después de los ajustes\")\n",
    "        else:\n",
    "            print(\"Aún no se detectan GPUs. El entrenamiento usará CPU y será significativamente más lento.\")\n",
    "\n",
    "def load_data(dataset_path, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Loads precomputed mel-spectrogram images and extracts labels from folder names.\n",
    "    Returns both raw data and data split into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(dataset_path))  # Get emotion categories\n",
    "    \n",
    "    for label in class_names:\n",
    "        class_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        files = glob.glob(os.path.join(class_path, '*.png'))\n",
    "        print(f\"Found {len(files)} images for class '{label}'.\")\n",
    "        \n",
    "        for file in files:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "            img = cv2.resize(img, img_size)  # Resize to standard size\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "    y = np.array(y)\n",
    "    return X, y, class_names\n",
    "\n",
    "def augment_data(X, y, augmentation_factor=0.3):\n",
    "    \"\"\"Apply simple data augmentation to improve model generalization\"\"\"\n",
    "    aug_X = []\n",
    "    aug_y = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        # Original sample\n",
    "        aug_X.append(X[i])\n",
    "        aug_y.append(y[i])\n",
    "        \n",
    "        # Apply time stretching (horizontal scaling)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            stretched = cv2.resize(X[i], (int(X[i].shape[1] * 1.1), X[i].shape[0]))\n",
    "            stretched = cv2.resize(stretched, (X[i].shape[1], X[i].shape[0]))\n",
    "            aug_X.append(stretched)\n",
    "            aug_y.append(y[i])\n",
    "        \n",
    "        # Apply frequency masking (random horizontal lines masked)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            masked = X[i].copy()\n",
    "            num_masks = np.random.randint(1, 4)\n",
    "            for _ in range(num_masks):\n",
    "                freq_width = np.random.randint(5, 15)\n",
    "                freq_start = np.random.randint(0, X[i].shape[0] - freq_width)\n",
    "                masked[freq_start:freq_start+freq_width, :] = 0\n",
    "            aug_X.append(masked)\n",
    "            aug_y.append(y[i])\n",
    "    \n",
    "    return np.array(aug_X), np.array(aug_y)\n",
    "\n",
    "def preprocess_labels(y):\n",
    "    \"\"\"Encodes string labels into numerical one-hot vectors.\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "    return y_onehot, le\n",
    "\n",
    "def build_hybrid_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a hybrid CNN-LSTM model that leverages both spatial and temporal features.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First, add CNN layers to extract spatial features\n",
    "    model.add(Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Reshape for LSTM - treat rows as time steps\n",
    "    model.add(Reshape((32, 32*64)))\n",
    "    \n",
    "    # LSTM layers for temporal features\n",
    "    model.add(RNN(LSTMCell(128), return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(RNN(LSTMCell(64)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Classification layers\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def verify_gpu_availability():\n",
    "    \"\"\"Verifica la disponibilidad de GPU usando múltiples métodos\"\"\"\n",
    "    print(\"\\n===== VERIFICACIÓN DE GPU =====\")\n",
    "    \n",
    "    # Verificar con TensorFlow\n",
    "    print(\"\\n1. Verificación con TensorFlow:\")\n",
    "    tf_gpus = tf.config.list_physical_devices('GPU')\n",
    "    if tf_gpus:\n",
    "        print(f\"   ✓ TensorFlow detectó {len(tf_gpus)} GPU(s):\")\n",
    "        for i, gpu in enumerate(tf_gpus):\n",
    "            print(f\"     - GPU {i}: {gpu}\")\n",
    "    else:\n",
    "        print(\"   ✕ TensorFlow no detectó ninguna GPU\")\n",
    "    \n",
    "    # Verificar con variables de entorno\n",
    "    print(\"\\n2. Variables de entorno:\")\n",
    "    cuda_visible = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"No definido\")\n",
    "    print(f\"   CUDA_VISIBLE_DEVICES = {cuda_visible}\")\n",
    "    \n",
    "    # Verificar con nvidia-smi\n",
    "    print(\"\\n3. Verificación con nvidia-smi:\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        nvidia_smi = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"   ✓ nvidia-smi detectó GPU:\")\n",
    "        # Mostrar solo las primeras líneas para no saturar la salida\n",
    "        lines = nvidia_smi.split('\\n')\n",
    "        for line in lines[:10]:\n",
    "            print(f\"     {line}\")\n",
    "        if len(lines) > 10:\n",
    "            print(\"     ...\")\n",
    "    except:\n",
    "        print(\"   ✕ No se pudo ejecutar nvidia-smi\")\n",
    "    \n",
    "    # Verificar disponibilidad de CUDA\n",
    "    print(\"\\n4. Verificación de CUDA en TensorFlow:\")\n",
    "    print(f\"   Compilado con CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "    print(f\"   Dispositivos GPU disponibles: {tf.test.is_gpu_available()}\")\n",
    "    \n",
    "    # Comprobar si se puede ejecutar operaciones en GPU\n",
    "    print(\"\\n5. Prueba de operación en GPU:\")\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"   ✓ Operación exitosa en GPU: Resultado de forma {c.shape}\")\n",
    "            print(f\"   Dispositivo usado: {c.device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✕ Error al ejecutar operación en GPU: {e}\")\n",
    "    \n",
    "    print(\"\\n==============================\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Verificar disponibilidad de GPU antes de comenzar\n",
    "    verify_gpu_availability()\n",
    "    \n",
    "    # Configurar GPU\n",
    "    configure_gpu()\n",
    "    \n",
    "    # Verificar que TensorFlow esté usando la GPU\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name:\n",
    "        print(f\"GPU encontrada: {device_name}\")\n",
    "    else:\n",
    "        print(\"ADVERTENCIA: No se encontró GPU. El entrenamiento será más lento.\")\n",
    "    \n",
    "    # Configuration parameters\n",
    "    dataset_path = 'Ravdess_mel_specs'  # Update with your dataset path\n",
    "    img_size = (128, 128)\n",
    "    test_size = 0.15\n",
    "    val_size = 0.15\n",
    "    batch_size = 64  # Optimizado para RTX 3060 Ti\n",
    "    max_epochs = 100\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y, class_names = load_data(dataset_path, img_size)\n",
    "    print(f\"Original dataset: {X.shape[0]} samples\")\n",
    "    \n",
    "    # Apply data augmentation\n",
    "    X_aug, y_aug = augment_data(X, y)\n",
    "    print(f\"Augmented dataset: {X_aug.shape[0]} samples\")\n",
    "    \n",
    "    # Encode labels\n",
    "    y_onehot, le = preprocess_labels(y_aug)\n",
    "    \n",
    "    # Create train, validation, and test sets\n",
    "    # First split off the test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_aug, y_onehot, test_size=test_size, random_state=42, \n",
    "        stratify=np.argmax(y_onehot, axis=1)\n",
    "    )\n",
    "    \n",
    "    # Then split the remaining data into train and validation\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, random_state=42,\n",
    "        stratify=np.argmax(y_temp, axis=1)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]}, Validation: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Build and compile the model\n",
    "    input_shape = X_train.shape[1:]  # (128, 128)\n",
    "    num_classes = y_onehot.shape[1]\n",
    "    model = build_hybrid_model(input_shape, num_classes)\n",
    "    \n",
    "    # Use a learning rate schedule with warmup\n",
    "    initial_lr = 0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Crear los generadores de datos para aprovechar mejor la GPU\n",
    "    # Aumentar el tamaño del buffer para mejor aleatorización\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=2048).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Callbacks for training\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_emotion_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Clase para monitorear el rendimiento de GPU durante el entrenamiento\n",
    "    class GPUMonitor(Callback):\n",
    "        def __init__(self, log_every=5):\n",
    "            super(GPUMonitor, self).__init__()\n",
    "            self.log_every = log_every\n",
    "            self.epoch_times = []\n",
    "            \n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "            \n",
    "            # Solo mostrar info de GPU cada log_every épocas\n",
    "            if epoch % self.log_every == 0:\n",
    "                try:\n",
    "                    # Intenta obtener información de la GPU usando subprocess\n",
    "                    import subprocess\n",
    "                    gpu_info = subprocess.check_output('nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits', \n",
    "                                                        shell=True).decode('utf-8')\n",
    "                    print(f\"\\nEstado de GPU (Época {epoch}):\")\n",
    "                    for i, line in enumerate(gpu_info.strip().split('\\n')):\n",
    "                        util, mem_used, mem_total = line.split(', ')\n",
    "                        print(f\"  GPU {i}: Utilización {util}%, Memoria: {mem_used}/{mem_total} MB\")\n",
    "                except:\n",
    "                    print(\"No se pudo obtener información de la GPU\")\n",
    "        \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.epoch_times.append(epoch_time)\n",
    "            \n",
    "            if epoch % self.log_every == 0 or epoch == self.params['epochs'] - 1:\n",
    "                avg_time = np.mean(self.epoch_times[-self.log_every:])\n",
    "                print(f\"\\nTiempo de época: {epoch_time:.2f}s, Promedio últimas {min(self.log_every, len(self.epoch_times))} épocas: {avg_time:.2f}s\")\n",
    "                print(f\"Tiempo estimado restante: {avg_time * (self.params['epochs'] - epoch - 1):.2f}s\")\n",
    "    \n",
    "    # Añadir monitor de GPU a los callbacks\n",
    "    gpu_monitor = GPUMonitor(log_every=2)\n",
    "    \n",
    "    # Registrar hora de inicio para cálculo de tiempo total\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model using tf.data.Dataset\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=max_epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[early_stop, checkpoint, reduce_lr, gpu_monitor],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Calcular tiempo total de entrenamiento\n",
    "    total_time = time.time() - start_time\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"\\nTiempo total de entrenamiento: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_results = model.evaluate(test_dataset)\n",
    "    test_metrics = dict(zip(model.metrics_names, test_results))\n",
    "    \n",
    "    print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "    \n",
    "    # Generate predictions and evaluate in detail\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true_classes, \n",
    "        y_pred_classes, \n",
    "        target_names=[le.inverse_transform([i])[0] for i in range(num_classes)]\n",
    "    ))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Guardar un resumen del entrenamiento\n",
    "    with open('training_summary.txt', 'w') as f:\n",
    "        f.write(f\"Entrenamiento completado: {datetime.datetime.now()}\\n\")\n",
    "        f.write(f\"Tiempo de entrenamiento: {int(hours)}h {int(minutes)}m {seconds:.2f}s\\n\")\n",
    "        f.write(f\"Métricas de prueba:\\n\")\n",
    "        for name, value in test_metrics.items():\n",
    "            f.write(f\"  {name}: {value:.4f}\\n\")\n",
    "        f.write(f\"Modelo guardado en: best_emotion_model.h5\\n\")\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\__init__.py:2225\u001b[39m\n\u001b[32m   2222\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2224\u001b[39m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2227\u001b[39m \u001b[38;5;66;03m# If you are seeing this, it means that this call site was not checked if\u001b[39;00m\n\u001b[32m   2228\u001b[39m \u001b[38;5;66;03m# the memory format could be preserved, and it was switched to old default\u001b[39;00m\n\u001b[32m   2229\u001b[39m \u001b[38;5;66;03m# behaviour of contiguous\u001b[39;00m\n\u001b[32m   2230\u001b[39m legacy_contiguous_format = contiguous_format  \u001b[38;5;66;03m# defined by _C._initExtension()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1178\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1149\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:936\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1032\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1131\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import psutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Verificar disponibilidad de GPU\n",
    "def verify_gpu_availability():\n",
    "    \"\"\"Verifica la disponibilidad de GPU para PyTorch\"\"\"\n",
    "    print(\"\\n===== VERIFICACIÓN DE GPU (PyTorch) =====\")\n",
    "    \n",
    "    # Verificar si CUDA está disponible\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA disponible: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        print(f\"Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        \n",
    "        # Verificar versión de CUDA\n",
    "        print(f\"Versión de CUDA: {torch.version.cuda}\")\n",
    "        \n",
    "        # Verificar dispositivo actual\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"Dispositivo actual: {current_device} ({torch.cuda.get_device_name(current_device)})\")\n",
    "    \n",
    "    # Verificar con nvidia-smi\n",
    "    print(\"\\nVerificación con nvidia-smi:\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        nvidia_smi = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"nvidia-smi detectó GPU:\")\n",
    "        # Mostrar solo las primeras líneas\n",
    "        lines = nvidia_smi.split('\\n')\n",
    "        for line in lines[:10]:\n",
    "            print(f\"  {line}\")\n",
    "        if len(lines) > 10:\n",
    "            print(\"  ...\")\n",
    "    except:\n",
    "        print(\"No se pudo ejecutar nvidia-smi\")\n",
    "    \n",
    "    # Probar una operación básica en GPU si está disponible\n",
    "    if cuda_available:\n",
    "        print(\"\\nPrueba de operación en GPU:\")\n",
    "        x = torch.rand(1000, 1000).cuda()\n",
    "        y = torch.rand(1000, 1000).cuda()\n",
    "        start_time = time.time()\n",
    "        z = torch.matmul(x, y)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Operación de multiplicación matricial completada en {elapsed:.4f} segundos\")\n",
    "        print(f\"Dispositivo del tensor: {z.device}\")\n",
    "    \n",
    "    print(\"\\n==============================\")\n",
    "    return cuda_available\n",
    "\n",
    "# Clase para cargar datos de imágenes de espectrogramas\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Añadir dimensión de canal para imágenes en escala de grises\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1])\n",
    "        return torch.from_numpy(image).float(), torch.from_numpy(label).long()\n",
    "\n",
    "def load_data(dataset_path, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Carga imágenes de espectrogramas y extrae etiquetas de los nombres de carpetas.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(dataset_path))\n",
    "    \n",
    "    for idx, label in enumerate(class_names):\n",
    "        class_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        files = glob.glob(os.path.join(class_path, '*.png'))\n",
    "        print(f\"Encontradas {len(files)} imágenes para la clase '{label}'.\")\n",
    "        \n",
    "        for file in files:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            X.append(img)\n",
    "            y.append(idx)  # Guardar índice numérico directamente\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32) / 255.0  # Normalizar valores de píxeles\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    return X, y, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\__init__.py:2593\u001b[39m\n\u001b[32m   2589\u001b[39m     torch_module_name = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[34m__name__\u001b[39m, device_type])\n\u001b[32m   2590\u001b[39m     sys.modules[torch_module_name] = module\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   2594\u001b[39m     export \u001b[38;5;28;01mas\u001b[39;00m export,\n\u001b[32m   2595\u001b[39m     func \u001b[38;5;28;01mas\u001b[39;00m func,\n\u001b[32m   2596\u001b[39m     library \u001b[38;5;28;01mas\u001b[39;00m library,\n\u001b[32m   2597\u001b[39m     return_types \u001b[38;5;28;01mas\u001b[39;00m return_types,\n\u001b[32m   2598\u001b[39m )\n\u001b[32m   2599\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m cond, while_loop \u001b[38;5;28;01mas\u001b[39;00m while_loop\n\u001b[32m   2600\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\func\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     vjp,\n\u001b[32m      3\u001b[39m     jvp,\n\u001b[32m      4\u001b[39m     jacrev,\n\u001b[32m      5\u001b[39m     jacfwd,\n\u001b[32m      6\u001b[39m     hessian,\n\u001b[32m      7\u001b[39m     functionalize,\n\u001b[32m      8\u001b[39m     linearize\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m grad, grad_and_value\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_call\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_call, stack_module_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\_functorch\\eager_transforms.py:33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m argnums_t, exposed_in\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionalTensor\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m const_fold\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproxy_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_fx\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pytree \u001b[38;5;28;01mas\u001b[39;00m pytree\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\fx\\experimental\\const_fold.py:17\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msplit_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_module\n\u001b[32m     10\u001b[39m __all__ = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFoldedGraphModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mget_unique_attr_name_in_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msplit_const_subgraphs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFoldedGraphModule\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfx\u001b[49m.GraphModule):\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    FoldedGraphModule is a GraphModule which also contains another\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    `const_subgraph_module` representing a subgraph which has all const attr\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m    on which attrs.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     29\u001b[39m         root: torch.nn.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m         device_for_folded_attrs: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     ):\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import psutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Verificar disponibilidad de GPU\n",
    "def verify_gpu_availability():\n",
    "    \"\"\"Verifica la disponibilidad de GPU para PyTorch\"\"\"\n",
    "    print(\"\\n===== VERIFICACIÓN DE GPU (PyTorch) =====\")\n",
    "    \n",
    "    # Verificar si CUDA está disponible\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA disponible: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        print(f\"Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        \n",
    "        # Verificar versión de CUDA\n",
    "        print(f\"Versión de CUDA: {torch.version.cuda}\")\n",
    "        \n",
    "        # Verificar dispositivo actual\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"Dispositivo actual: {current_device} ({torch.cuda.get_device_name(current_device)})\")\n",
    "    \n",
    "    # Verificar con nvidia-smi\n",
    "    print(\"\\nVerificación con nvidia-smi:\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        nvidia_smi = subprocess.check_output(\"nvidia-smi\", shell=True).decode('utf-8')\n",
    "        print(\"nvidia-smi detectó GPU:\")\n",
    "        # Mostrar solo las primeras líneas\n",
    "        lines = nvidia_smi.split('\\n')\n",
    "        for line in lines[:10]:\n",
    "            print(f\"  {line}\")\n",
    "        if len(lines) > 10:\n",
    "            print(\"  ...\")\n",
    "    except:\n",
    "        print(\"No se pudo ejecutar nvidia-smi\")\n",
    "    \n",
    "    # Probar una operación básica en GPU si está disponible\n",
    "    if cuda_available:\n",
    "        print(\"\\nPrueba de operación en GPU:\")\n",
    "        x = torch.rand(1000, 1000).cuda()\n",
    "        y = torch.rand(1000, 1000).cuda()\n",
    "        start_time = time.time()\n",
    "        z = torch.matmul(x, y)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Operación de multiplicación matricial completada en {elapsed:.4f} segundos\")\n",
    "        print(f\"Dispositivo del tensor: {z.device}\")\n",
    "    \n",
    "    print(\"\\n==============================\")\n",
    "    return cuda_available\n",
    "\n",
    "\n",
    "# Clase para cargar datos de imágenes de espectrogramas\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Añadir dimensión de canal para imágenes en escala de grises\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1])\n",
    "        return torch.from_numpy(image).float(), torch.from_numpy(label).long()\n",
    "    \n",
    "\n",
    "def load_data(dataset_path, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Carga imágenes de espectrogramas y extrae etiquetas de los nombres de carpetas.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(dataset_path))\n",
    "    \n",
    "    for idx, label in enumerate(class_names):\n",
    "        class_path = os.path.join(dataset_path, label)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        files = glob.glob(os.path.join(class_path, '*.png'))\n",
    "        print(f\"Encontradas {len(files)} imágenes para la clase '{label}'.\")\n",
    "        \n",
    "        for file in files:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            X.append(img)\n",
    "            y.append(idx)  # Guardar índice numérico directamente\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32) / 255.0  # Normalizar valores de píxeles\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    return X, y, class_names\n",
    "\n",
    "\n",
    "def augment_data(X, y, augmentation_factor=0.3):\n",
    "    \"\"\"Aplica aumentación simple de datos para mejorar la generalización del modelo\"\"\"\n",
    "    aug_X = []\n",
    "    aug_y = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        # Muestra original\n",
    "        aug_X.append(X[i])\n",
    "        aug_y.append(y[i])\n",
    "        \n",
    "        # Aplicar estiramiento de tiempo (escalado horizontal)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            stretched = cv2.resize(X[i], (int(X[i].shape[1] * 1.1), X[i].shape[0]))\n",
    "            stretched = cv2.resize(stretched, (X[i].shape[1], X[i].shape[0]))\n",
    "            aug_X.append(stretched)\n",
    "            aug_y.append(y[i])\n",
    "        \n",
    "        # Aplicar enmascaramiento de frecuencia (líneas horizontales aleatorias enmascaradas)\n",
    "        if np.random.random() < augmentation_factor:\n",
    "            masked = X[i].copy()\n",
    "            num_masks = np.random.randint(1, 4)\n",
    "            for _ in range(num_masks):\n",
    "                freq_width = np.random.randint(5, 15)\n",
    "                freq_start = np.random.randint(0, X[i].shape[0] - freq_width)\n",
    "                masked[freq_start:freq_start+freq_width, :] = 0\n",
    "            aug_X.append(masked)\n",
    "            aug_y.append(y[i])\n",
    "    \n",
    "    return np.array(aug_X), np.array(aug_y)\n",
    "\n",
    "\n",
    "# Clase para el modelo híbrido CNN-LSTM\n",
    "class HybridCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(HybridCNNLSTM, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales para extraer características espaciales\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Calcular tamaño después de las capas convolucionales y pooling\n",
    "        conv_output_size = input_shape[0] // 4  # después de 2 capas de pooling\n",
    "        \n",
    "        # Capas LSTM para características temporales\n",
    "        self.lstm1 = nn.LSTM(input_size=conv_output_size*64, hidden_size=128, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Capas de clasificación\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # Guardar tamaño para reshape\n",
    "        self.conv_output_size = conv_output_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Parte CNN\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Reshape para LSTM: (batch, channels, height, width) -> (batch, time_steps, features)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.conv_output_size, -1)\n",
    "        \n",
    "        # Parte LSTM\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Tomar el último estado oculto\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Clasificación\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Clase para monitoreo de GPU\n",
    "class GPUMonitor:\n",
    "    def __init__(self, log_every=5):\n",
    "        self.log_every = log_every\n",
    "        self.epoch_times = []\n",
    "    \n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "        # Mostrar info de GPU cada log_every épocas\n",
    "        if epoch % self.log_every == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                # Obtener estadísticas de la GPU\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    print(f\"\\nEstado de GPU {i} (Época {epoch}):\")\n",
    "                    print(f\"  Nombre: {torch.cuda.get_device_name(i)}\")\n",
    "                    print(f\"  Memoria asignada: {torch.cuda.memory_allocated(i) / 1e6:.2f} MB\")\n",
    "                    print(f\"  Memoria reservada: {torch.cuda.memory_reserved(i) / 1e6:.2f} MB\")\n",
    "                    \n",
    "                    # Intentar obtener información adicional con nvidia-smi\n",
    "                    try:\n",
    "                        import subprocess\n",
    "                        gpu_info = subprocess.check_output(\n",
    "                            f'nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits --id={i}', \n",
    "                            shell=True).decode('utf-8').strip()\n",
    "                        util, mem_used, mem_total = gpu_info.split(', ')\n",
    "                        print(f\"  Utilización: {util}%, Memoria: {mem_used}/{mem_total} MB\")\n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                print(\"\\nNo se detectó GPU, usando CPU\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch, total_epochs):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        if epoch % self.log_every == 0 or epoch == total_epochs - 1:\n",
    "            avg_time = np.mean(self.epoch_times[-self.log_every:])\n",
    "            print(f\"\\nTiempo de época: {epoch_time:.2f}s, Promedio últimas {min(self.log_every, len(self.epoch_times))} épocas: {avg_time:.2f}s\")\n",
    "            print(f\"Tiempo estimado restante: {avg_time * (total_epochs - epoch - 1):.2f}s\")\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=100):\n",
    "    # Inicializar monitor de GPU\n",
    "    gpu_monitor = GPUMonitor(log_every=2)\n",
    "    \n",
    "    # Listas para almacenar historial de entrenamiento\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Mejor modelo y parámetros\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    early_stop_patience = 20\n",
    "    \n",
    "    # Registrar tiempo de inicio para cálculo de tiempo total\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # Notificar inicio de época\n",
    "        gpu_monitor.on_epoch_begin(epoch)\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Poner a cero los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass y optimización\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Actualizar estadísticas\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Mostrar progreso\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Época: {epoch+1}/{num_epochs} | Batch: {batch_idx+1}/{len(train_loader)} | '\n",
    "                      f'Loss: {loss.item():.4f} | Acc: {100.*train_correct/train_total:.2f}%')\n",
    "        \n",
    "        # Calcular métricas de entrenamiento\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluación en conjunto de validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Calcular métricas de validación\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Imprimir resultados de la época\n",
    "        print(f'Época {epoch+1}/{num_epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '\n",
    "              f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Actualizar scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"Nuevo mejor modelo guardado con pérdida de validación: {val_loss:.4f}\")\n",
    "            torch.save(best_model_state, 'best_emotion_model.pt')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Época sin mejora: {epochs_no_improve}/{early_stop_patience}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if epochs_no_improve >= early_stop_patience:\n",
    "                print(f\"Deteniendo entrenamiento temprano en época {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Notificar fin de época\n",
    "        gpu_monitor.on_epoch_end(epoch, num_epochs)\n",
    "    \n",
    "    # Calcular tiempo total de entrenamiento\n",
    "    total_time = time.time() - start_time\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"\\nTiempo total de entrenamiento: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "    \n",
    "    # Cargar el mejor modelo\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Devolver historial y mejor modelo\n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_acc': train_accs,\n",
    "        'val_acc': val_accs\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, class_names):\n",
    "    \"\"\"Evalúa el modelo en el conjunto de prueba\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Guardar etiquetas y predicciones para métricas\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Calcular métricas\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100. * test_correct / test_total\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Imprimir informe de clasificación\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_targets, all_predictions, target_names=class_names))\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    \n",
    "    # Devolver métricas\n",
    "    metrics = {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_acc,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Grafica el historial de entrenamiento\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'])\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.title('Precisión del Modelo')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Precisión (%)')\n",
    "    plt.legend(['Entrenamiento', 'Validación'], loc='lower right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Pérdida del Modelo')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend(['Entrenamiento', 'Validación'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"Grafica la matriz de confusión\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Mostrar valores en las celdas\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta Real')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Verificar disponibilidad de GPU\n",
    "    cuda_available = verify_gpu_availability()\n",
    "    \n",
    "    # Configurar dispositivo\n",
    "    device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "    \n",
    "    # Parámetros de configuración\n",
    "    dataset_path = 'Ravdess_mel_specs'  # Actualizar con la ruta de tu conjunto de datos\n",
    "    img_size = (128, 128)\n",
    "    test_size = 0.15\n",
    "    val_size = 0.15\n",
    "    batch_size = 64  # Optimizado para RTX 3060 Ti\n",
    "    max_epochs = 100\n",
    "    \n",
    "    # Cargar y preprocesar datos\n",
    "    X, y, class_names = load_data(dataset_path, img_size)\n",
    "    print(f\"Dataset original: {X.shape[0]} muestras\")\n",
    "    \n",
    "    # Aplicar aumentación de datos\n",
    "    X_aug, y_aug = augment_data(X, y)\n",
    "    print(f\"Dataset aumentado: {X_aug.shape[0]} muestras\")\n",
    "    \n",
    "    # Dividir en conjuntos de entrenamiento, validación y prueba\n",
    "    # Primero separar el conjunto de prueba\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=test_size, random_state=42, \n",
    "        stratify=y_aug\n",
    "    )\n",
    "    \n",
    "    # Luego dividir los datos restantes en entrenamiento y validación\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Entrenamiento: {X_train.shape[0]}, Validación: {X_val.shape[0]}, Prueba: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Crear conjuntos de datos y dataloaders\n",
    "    train_dataset = MelSpectrogramDataset(X_train, y_train)\n",
    "    val_dataset = MelSpectrogramDataset(X_val, y_val)\n",
    "    test_dataset = MelSpectrogramDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Construir modelo\n",
    "    input_shape = (128, 128)  # Tamaño de imagen\n",
    "    num_classes = len(class_names)\n",
    "    model = HybridCNNLSTM(input_shape, num_classes).to(device)\n",
    "    \n",
    "    # Imprimir resumen del modelo\n",
    "    print(model)\n",
    "    # Contar parámetros\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Número total de parámetros: {total_params:,}\")\n",
    "    \n",
    "    # Definir función de pérdida y optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6, verbose=True)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nComenzando entrenamiento...\\n\")\n",
    "    trained_model, history = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=max_epochs\n",
    "    )\n",
    "    \n",
    "    # Evaluar en conjunto de prueba\n",
    "    print(\"\\nEvaluando modelo en conjunto de prueba...\\n\")\n",
    "    test_metrics = evaluate_model(trained_model, test_loader, criterion, device, class_names)\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    plot_training_history(history)\n",
    "    plot_confusion_matrix(test_metrics['confusion_matrix'], class_names)\n",
    "    \n",
    "    # Guardar un resumen del entrenamiento\n",
    "    with open('training_summary.txt', 'w') as f:\n",
    "        f.write(f\"Entrenamiento completado: {datetime.datetime.now()}\\n\")\n",
    "        f.write(f\"Dispositivo utilizado: {device}\\n\")\n",
    "        f.write(f\"Métricas de prueba:\\n\")\n",
    "        f.write(f\"  Pérdida: {test_metrics['loss']:.4f}\\n\")\n",
    "        f.write(f\"  Precisión: {test_metrics['accuracy']:.2f}%\\n\")\n",
    "        f.write(f\"Modelo guardado en: best_emotion_model.pt\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\__init__.py:2593\u001b[39m\n\u001b[32m   2589\u001b[39m     torch_module_name = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[34m__name__\u001b[39m, device_type])\n\u001b[32m   2590\u001b[39m     sys.modules[torch_module_name] = module\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   2594\u001b[39m     export \u001b[38;5;28;01mas\u001b[39;00m export,\n\u001b[32m   2595\u001b[39m     func \u001b[38;5;28;01mas\u001b[39;00m func,\n\u001b[32m   2596\u001b[39m     library \u001b[38;5;28;01mas\u001b[39;00m library,\n\u001b[32m   2597\u001b[39m     return_types \u001b[38;5;28;01mas\u001b[39;00m return_types,\n\u001b[32m   2598\u001b[39m )\n\u001b[32m   2599\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m cond, while_loop \u001b[38;5;28;01mas\u001b[39;00m while_loop\n\u001b[32m   2600\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\func\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     vjp,\n\u001b[32m      3\u001b[39m     jvp,\n\u001b[32m      4\u001b[39m     jacrev,\n\u001b[32m      5\u001b[39m     jacfwd,\n\u001b[32m      6\u001b[39m     hessian,\n\u001b[32m      7\u001b[39m     functionalize,\n\u001b[32m      8\u001b[39m     linearize\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m grad, grad_and_value\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_call\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_call, stack_module_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\_functorch\\eager_transforms.py:33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m argnums_t, exposed_in\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionalTensor\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m const_fold\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproxy_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_fx\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pytree \u001b[38;5;28;01mas\u001b[39;00m pytree\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eel20\\Desktop\\Sentiment-Analysis-Speech-Emotion-Recognition\\.venv\\Lib\\site-packages\\torch\\fx\\experimental\\const_fold.py:17\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msplit_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_module\n\u001b[32m     10\u001b[39m __all__ = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFoldedGraphModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mget_unique_attr_name_in_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msplit_const_subgraphs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFoldedGraphModule\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfx\u001b[49m.GraphModule):\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    FoldedGraphModule is a GraphModule which also contains another\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    `const_subgraph_module` representing a subgraph which has all const attr\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m    on which attrs.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     29\u001b[39m         root: torch.nn.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m         device_for_folded_attrs: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     ):\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    \"\"\"\n",
    "    Imprime información del sistema de entrenamiento\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"🖥️ Dispositivo de entrenamiento: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🚀 Número de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "        print(f\"🔋 Nombre de GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Información de memoria\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convertir a GB\n",
    "        print(f\"💾 Memoria total de GPU: {total_memory:.2f} GB\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print_system_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
